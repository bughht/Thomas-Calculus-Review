\documentclass{article}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{geometry}
\usepackage{pgfplots}
\geometry{a4paper,left=3.5cm,right=3.5cm,top=3cm,bottom=4cm}
\title{Thomas Calculus Review}
\author{Harryhht}
\date{\today}

\begin{document}
    \maketitle
    \newpage
    \tableofcontents
    \newpage
    \section{Function}
        \subsection{Function and their Graphs}
            \textit{DEFINITION:
            A function $f$ from a set $D$ to a set $Y$ is a rule that assigns a unique (single) element $f(x)\in Y$ to each element $x \in D$.}
            \paragraph{Linear Functions} $f(x)=mx+ba$
            \paragraph{Power Functions} $f(x)=x^a$ where $a$ is a constant
            \paragraph{Polynomials} $p(x)=a_n x^n+ a_{n-1} x^{n-1}+a_{n-2} x^{n-2}+\cdots+a_1 x + a_0$ where n is a nonnegative integer and $a_0, a_1,\cdots a_n$ are real constants (called the \textbf{coefficients} of the polynomial) 
            \paragraph{Rational Functions} $f(x)=\cfrac{p(x)}{q(x)}$ where $p$ and $q$ are polynomials.
            \paragraph{Algebraic Functions} Any function constructed from polynomials using algebraic operations lies within the class of \textbf{algebraic functions}.
            \paragraph{Trigonometric Function}
            \begin{equation}
                \left\{
                \begin{aligned}
                    f(x)=\sin(x)\\
                    f(x)=\cos(x)\\
                    f(x)=\tan(x)\\
                    f(x)=\csc(x)\\
                    f(x)=\sec(x)\\
                    f(x)=\cot(x)\\
                \end{aligned}
                \right.
            \end{equation}
            \paragraph{Exponential Functions} $f(x)=a^x$ where the base $a$ is a positive constant and $a\ne 1$
            \paragraph{Logarithmic Functions} $f(x)=\log _{a}x$ 
            \paragraph{Transcendental Functions} Functions that are not algebraic.

        \subsection{Combining Functions; Shifting and Scaling Graphs}
            \paragraph{Sums,Differences,Products, and Quotients}
            \begin{equation}
                \begin{cases}
                    (f+g)(x)=f(x)+ g(x)\\
                    (f-g)(x)=f(x)- g(x)\\
                    (fg)(x)=f(x)g(x)\\
                    (\cfrac{f}{g})(x)=\cfrac{f(x)}{g(x)}\\
                    (cf)(x)=cf(x)
                \end{cases}
            \end{equation} 

            \paragraph{Composite Functions}
            \subparagraph{DEFINITION} If $f$ and $g$ are functions, the \textbf{composite} of function $f\circ g$ ("$f$ composed with $g$") is defined by
            \begin{equation}
                (f\circ g)(x)=f(g) 
            \end{equation}

            \paragraph{Shifting a Graph of a Function}
            Vertical Shifts and Horizontal Shifts
            
            \paragraph{Scaling and Reflecting a Graph of a Function}
            Vertical and Horizontal Scaling and Reflecting Formulas
        \subsection{Trigonometric Function}

            \paragraph{Angles} are measured in degrees or radians.\\
            1 radian $= \cfrac{180}{\pi} (\approx 57.3)$degrees
            
            \paragraph{The Six Basic Trigonometric Functions}
            \begin{equation}
                \begin{cases}
                    \sin \theta=\cfrac{y}{r}\\
                    \csc \theta=\cfrac{r}{y}\\
                    \cos \theta=\cfrac{x}{r}\\
                    \sec \theta=\cfrac{r}{x}\\
                    \tan \theta=\cfrac{y}{x}\\
                    \cot \theta=\cfrac{x}{y}
                \end{cases}    
                \begin{cases}
                    \tan \theta=\cfrac{\sin \theta}{\cos \theta}\\
                    \cot \theta=\cfrac{1}{\tan \theta}\\
                    \sec \theta=\cfrac{1}{\cos \theta}\\
                    \csc \theta=\cfrac{1}{\sin \theta}
                \end{cases}
            \end{equation}
            \paragraph{Trigonometric Identities}
            \[x=r\cos \theta \qquad y=r\sin \theta\]
            \[\sin ^2 \theta + \cos ^2 \theta =1\]
            \[1+ \tan ^2 \theta = \sec ^2 \theta\]
            \[1+ \cot ^2 \theta = \csc ^2 \theta\]
            \subparagraph{Addition Formulas}
            \begin{equation}
                \begin{aligned}
                    \cos (A+B)=\cos A \cos B- \sin A \sin B\\
                    \sin (A+B)=\sin A \cos B+ \cos A \sin B
                \end{aligned}
            \end{equation}
            
            \subparagraph{Double-Angle Formulas}
            \begin{equation}
                \begin{aligned}
                    \cos 2\theta &= \cos ^2 \theta - \sin ^2 \theta\\
                    \sin 2\theta &= 2\sin \theta \cos \theta
                \end{aligned}
            \end{equation}

            \subparagraph{Half-Angle Formulas}
            \begin{equation}
                \begin{aligned}
                    \cos ^2 \theta = \cfrac{1+\cos 2\theta}{2}\\
                    \sin ^2 \theta = \cfrac{1-\cos 2\theta}{2} 
                \end{aligned}  
            \end{equation}

            \subparagraph{The Law of Cosines}
            \[c^2=a^2+b^2-2ab\cos \theta\]

            \subparagraph{Two Special Inequalities}
            \[-\lvert \theta \rvert \le \sin \theta \le \lvert \theta \rvert\]
            \[-\lvert \theta \rvert \le 1-\cos \theta \le \lvert \theta \rvert\]

            \paragraph{Transformations of Trigonometric Graphs}
            \[y=af(b(x+c))+d\]
            $a$:Vertical stretch or compression;
            reflection about $y=d$ if negative\\
            $b$:Horizontal stretch or compression;
            reflection about $x=-c$ if negative\\
            $c$:Horizontal shift\\
            $d$:Vertical shift\\

    \newpage
    \section{Limits and Continuity}
        \subsection{Rates of Change and Tangents to Curves} 
            \[\cfrac{\Delta y}{\Delta x}\]
            \paragraph{DEFINITION} The \textbf{average rate of change} of $y=f(x)$ with respect to $x$ over the interval $[x_1,x_2]$ is
            \[\cfrac{\Delta y}{\Delta x}=\cfrac{f(x_2)-f(x_1)}{x_2-x_1}=\cfrac{f(x_1 +h)-f(x_1)}{h},\quad h\ne 0\]
            \subsection{Limit of a Function and Limit Laws}
                \paragraph{Limits of Function Value} $\lim\limits_{x\to c}f(x)=L$ (read ``the limit of $f(x)$ as $x$ approaches $c$ is $L$'')
                    \subparagraph{``Informal'' definition} The values of $f(x)$ are close to the number $L$ whenever is close to $c$ (on either side of $c$)
                \paragraph{The Limit Laws}
                    \subparagraph{Limit Laws} If $L$, $M$,$c$ and $k$ are real numbers and $\lim\limits_{x\to c}f(x)=L$\quad and \quad $\lim\limits_{x\to c} g(x)=M$, then
                    \begin{equation}
                        \begin{aligned}
                            \lim\limits_{x\to c} (f(x)+g(x))&=L+M\\
                            \lim\limits_{x\to c} (f(x)-g(x))&=L-M\\
                            \lim\limits_{x\to c} (f(x)\cdot g(x))&=L\cdot M\\
                            \lim\limits_{x\to c} (k\cdot f(x))&=k\cdot L\\
                            \lim\limits_{x\to c} \cfrac{f(x)}{g(x)}&= \cfrac{L}{M},\quad M\ne 0\\
                            \lim\limits_{x\to c}[f(x)]^n &=L^n\\
                            \lim\limits_{x\to c}\sqrt[n]{f(x)}&=\sqrt[n]{L}=L^{\frac{1}{n}}
                        \end{aligned}
                    \end{equation}
                    \subparagraph{Limits of Polynomials} If $P(x)=a_n x^n+a_{n-1}x^{n-1}+\cdots +a_0$, then 
                    \[\lim\limits_{x\to c}P(x)=P(c)=a_nc^n+a_{n-1}c^{n-1}+\cdots +a_0\]
                    \subparagraph{Limits of Rational Funcitions} If $P(x)$ and $Q(x)$ are polynomials and $Q(c)\ne 0$, then
                    \[\lim\limits_{x\to c} \cfrac{P(x)}{Q(x)}=\cfrac{P(c)}{Q(c)}\]
                \paragraph{The Sandwich Theorem}
                    Suppose that $g(x)\le f(x) \le h(x)$ for all $x$ in some open interval containing $c$, expect possibly at $x=c$ itself. Suppose also that 
                    \[\lim\limits_{x\to c}g(x)=\lim\limits_{x\to c}h(x)=L\]
                    Then $\lim\limits_{x\to c}f(x)=L$. 
                \subparagraph{Theorem} If $f(x)\le g(x)$ for all $x$ in some open interval containing $c$, expect possibly at $x=c$ itself, and the limits of $f$ and $g$ both exist as $x$ approaches $c$, then
                \[\lim\limits_{x\to c}f(x)\le \lim\limits_{x\to c}g(x)\]
            \subsection{The Precise Definition of a Limit}
                \paragraph{DEFINITION} Let $f(x)$ be defined on an open interval about $c$, except possibly at $c$ itself. We say that the \textbf{limit of $f(x)$ as $x$ approaches $c$ is the number $L$}, and write
                \[\lim\limits_{x\to c}f(x)=L\]
                if, for every number $\epsilon>0$, there exists a corresponding number $\delta>0$ such that for all $x$,
                \[0<\lvert x-c \rvert <\delta \quad \Rightarrow \quad \lvert f(x)-L \rvert <\epsilon\]
            \subsection{One-Sided Limits}
                \paragraph{Two-sided limits}right-hand limit and left-hand limit
                    \subparagraph{THEOREM} A function $f(x)$ has a limit as $x$ approaches $c$ if and only if it has left-hand and right-hand limits there and these one-sided limits are equal:
                    \[\lim\limits_{x\to c}f(x)=L\quad \Leftrightarrow \quad \lim\limits_{x\to c^-}f(x)=L\quad \text{and} \quad \lim\limits_{x\to c^+}f(x)=L\]
                    \subparagraph{Limits Involving $(\sin \theta)/\theta$}
                    \[\lim\limits_{\theta\to 0}\cfrac{\sin\theta}{\theta}=1\]
            \subsection{Continuity}
                \paragraph{DEFINITION} Let $c$ be a real number on the $x$-axis.\\
                The function $f$ is \textbf{Continuous at} $c$ if 
                \[\lim\limits_{x\to c}f(x)=f(c)\]
                The function $f$ is \textbf{right-continuous at $c$ (or continuous from the right)} if
                \[\lim\limits_{x\to c^+}f(x)=f(c)\]
                The function $f$ is \textbf{left-continuous at $c$ (or continuous from the left)} if
                \[\lim\limits_{x\to c^-}f(x)=f(c)\]
                \paragraph{Properties of Continuous Functions} If the functions $f$ and $g$ are continuous at $x=c$, then the following algebraic combinations are continuous at $x=c$.
                \begin{equation}
                    \begin{aligned}
                        f+g\\
                        f-g\\
                        k\cdot f\\
                        f\cdot g\\
                        f/g\\
                        f^g\\
                        \sqrt[n]{f}\\
                    \end{aligned}
                \end{equation}
                \paragraph{Composite of Continuous Functions} If $f$ is continuous at $c$ and $g$ is continuous at $f(c)$, then the composite $g\circ f$ is continuous at $c$.
                \paragraph{Limits of Continuous Functions} If $g$ is continuous at the point $b$ and $\lim\limits_{x\to c}f(x)=b$, then 
                \[\lim\limits_{x\to c}g(f(x))=g(b)=g(\lim\limits_{x\to c}f(x)).\]
                \paragraph{The Intermediate Value Theorem for Continuous Functions} If $f$ is a continuous function on a closed interval [$a$,$b$], and if $y_0$ is any value between $f(a)$ and $f(b)$, then $y_0=f(c)$ for some $c$ in [$a$,$b$].
            \subsection{Limits Involving Infinity; Asymptotes of Graphs}
                \[\lim\limits_{x\to \pm \infty}k=k\]
                \[\lim\limits_{x\to \pm \infty}\frac{1}{x}=0\]                    
                \subparagraph{EXAMPLE}
                \[\lim\limits_{x\to -\infty}\cfrac{11x+2}{2x^3-1}
                =\lim\limits_{x\to -\infty}\cfrac{(11/x^2)+(2/x^3)}{2-(1/x^3)}=\cfrac{0+0}{2-0}=0\]
                \paragraph{DEFINITION} A line $y=b$ is a \textbf{horizontal asymptote} of the graph of a function $y=f(x)$ if either
                \[\lim\limits_{x\to \infty}f(x)=b\quad\text{or}\quad\lim\limits_{x\to -\infty}f(x)=b\]
                \paragraph{EXAMPLE}
                \[\lim\limits_{x\to \infty}\sin\cfrac{1}{x}=\lim\limits_{t\to 0^+}\sin t=0\]
                \[\lim\limits_{x\to -\infty}x\sin \cfrac{1}{x}=\lim\limits_{t\to 0^+}\cfrac{\sin t}{t}=1\]
                \[\lim\limits_{x\to \infty}x\sin \cfrac{1}{x}=\lim\limits_{t\to 0^-}\cfrac{\sin t}{t}=1\]
                \begin{equation}
                    \begin{aligned}
                        \lim\limits_{x\to -\infty}\cfrac{2x^5-6x^4+1}{3x^2+x-7}
                        &=\lim\limits_{x\to -\infty}\cfrac{2x^3-6x^2+x^{-2}}{3+x^{-1}-7x^{-2}}\\
                        &=\lim\limits_{x\to -\infty}\cfrac{2x^2(x-3)+x^{-2}}{3+x^{-1}-7x^{-2}}\\
                        &=-\infty
                    \end{aligned}
                \end{equation}

    \newpage
    \section{Derivatives}
        \subsection{Tangents and the Derivative at a Point}
            \paragraph{DEFINITIONS} The \textbf{derivative of a function $f$ at a point $x_0$}, denoted $f'(x_0)$, is 
            \[m=\lim\limits_{h\to 0}\cfrac{f(x_0+h)-f(x_0)}{h}\quad\text{(provided the limit exists)}\]
            The \textbf{tangent line} to the curve at $P$ is the line through $P$ with this slope.
        \subsection{The Derivative as a Function}
            \paragraph{Calculating Derivatives from the Definition}
            The process of calculating a derivative is called differentiation. To emphasize the idea that differentiation is an operation performed on a function $y=f(x)$, we use the notation
            \[\frac{d}{dx}f(x)\]
            as another way to denote the derivative $f'(x)$.
            \subparagraph{Notations}
            \[f'{x}=y'=\cfrac{dy}{dx}=\cfrac{df}{dx}=\cfrac{d}{dx}f(x)=D(f)(x)=D_x f(x)\]
            \paragraph{Differentiable on an Interval; One-Sided Derivatives}
            \begin{equation}
                \begin{aligned}
                    \lim\limits_{h\to 0^+}\cfrac{f(a+h)-f(a)}{h}\quad&\textbf{Right-hand derivative at a}\\
                    \lim\limits_{h\to 0^-}\cfrac{f(h+h)-f(b)}{h}\quad&\textbf{Left-hand derivative at b}
                \end{aligned}  
            \end{equation}
            \textbf{When Does a Function \textit{Not} Have a Derivative at a Point?}\\
            1. a corner,where the one-sided derivatives differ.\\
            2. a cusp, where the slope of \textit{PQ} approaches $\infty$ from one side and $-\infty$ from the other.\\
            3. a vertical tangent, where the slope of $PQ$ approaches $\infty$ from both sides or approaches $-\infty$ from both sides.\\
            4. a \textit{discontinuity}.
            \paragraph{Differentiable Functions Are Continuous}
            \subparagraph{Differentiability Implies Continuity} If $f$ has a derivative at $x=c$, then $f$ is continuous at $x=c$.
        \subsection{Differentiation Rules}
            \paragraph{Power,Multiples, Sums, and Differences}
            \begin{equation}
                \begin{aligned}
                    \cfrac{d}{dx}(c)&=0\\
                    \cfrac{d}{dx}(x^n)&=nx^{n-1}\\
                    \cfrac{d}{dx}(cu)&=c\cfrac{du}{dx}\\
                    \text{so }(\cfrac{d}{dx}(cx^n)&=cnx^{n-1})\\
                    \cfrac{d}{dx}(u+v)&=\cfrac{du}{dx}+\cfrac{dv}{dx}\\
                    \cfrac{d}{dx}(uv)&=u\cfrac{dv}{dx}+v\cfrac{du}{dx}\\
                    \cfrac{d}{dx}(\cfrac{u}{v})&=\cfrac{v\cfrac{du}{dx}-u\cfrac{dv}{dx}}{v^2}\\
                \end{aligned}  
            \end{equation}
            \paragraph{Second- and Higher-Order Derivatives}
            \[f^{''}(x)=\cfrac{d^2y}{dx^2}=\cfrac{d}{dx}(\cfrac{dy}{dx})=\cfrac{dy'}{dx}=y^{''}=D^2(f)(x)={D_x}^2f(x)\]
        \subsection{The Derivative as a Rate of Change}
            \paragraph{Motion Along a Line: Displacement, Velocity, Speed, Acceleration, and Jerk}
            \[v(t)=\cfrac{ds}{dt}\]
            \[\text{Speed}=\lvert v(t) \rvert= \lvert \cfrac{ds}{dt} \rvert\]
            \[a(t)=\cfrac{dv}{dt}=\cfrac{d^2 s}{dt^2}\]
            \[j(t)=\cfrac{da}{dt}=\cfrac{d^3 s}{dt^3}\]
        \subsection{Derivatives of Trigonometric Functions}
            \begin{equation}
                \begin{aligned}
                    \cfrac{d}{dx}(\sin x)&=\cos x\\
                    \cfrac{d}{dx}(\cos x)&=-\sin x\\
                    \cfrac{d}{dx}(\tan x)&=\sec ^2 x\\
                    \cfrac{d}{dx}(\cot x)&=-\csc ^2 x\\
                    \cfrac{d}{dx}(\sec x)&=\sec x \tan x\\
                    \cfrac{d}{dx}(\csc x)&=-\csc x \cot x\\
                \end{aligned} 
            \end{equation}
        \subsection{The Chain Rule*}
            \[\cfrac{dy}{dx}=\cfrac{dy}{du}\cdot\cfrac{du}{dx}\]
            \[(f\circ g)'(x)=f'(g(x))\cdot g'(x)\]
            \paragraph{Power Chain Rule} Using chain rules in loop.
        \subsection{Implicit Differentiation}
            \paragraph{Implicitly Defined Functions}
                \subparagraph{EXAMPLE 1}
                    \begin{equation}
                        \begin{aligned}
                            y^2&=x\\
                            2y\cfrac{dy}{dx}&=1\\
                            \cfrac{dy}{dx}&=\cfrac{1}{2y}
                        \end{aligned}  
                    \end{equation}
                \subparagraph{EXAMPLE 2}
                    \begin{equation}
                        \begin{aligned}
                            x^2+y^2&=25\\
                            \cfrac{d}{dx}(x^2)+\cfrac{d}{dx}(y^2)&=\cfrac{d}{dx}(25)\\
                            2x+2y\cfrac{dy}{dx}&=0\\
                            \cfrac{dy}{dx}&=-\cfrac{x}{y}
                        \end{aligned} 
                    \end{equation}

                \subparagraph{EXAMPLE 3}
                    \begin{equation}
                        \begin{aligned}
                            y^2&=x^2+\sin xy\\
                            \cfrac{d}{dx}(y^2)&=\cfrac{d}{dx}(x^2)+\cfrac{d}{dx}(\sin xy)\\
                            2y\cfrac{dy}{dx}&=2x+(\cos xy)\cfrac{d}{dx}(xy)\\
                            2y\cfrac{dy}{dx}&=2x+(\cos xy)(y+x\cfrac{dy}{dx})\\
                            (2y-x\cos xy)\cfrac{dy}{dx}&=2x+y\cos xy\\
                            \cfrac{dy}{dx}&=\cfrac{2x+y\cos xy}{2y-x\cos xy}
                        \end{aligned}
                    \end{equation}
        \subsection{Related Rates}
            \paragraph{Related Rate Equations}
                \[V=\cfrac{4}{3}\pi r^3\]
                \[\cfrac{dV}{dt}=\cfrac{dV}{dr}\cfrac{dr}{dt}=4\pi r^2\cfrac{dr}{dt}\]
            \textbf{Strategy}\\
                \textit{1. Draw a Picture and name the variables and constants.}\\
                \textit{2. Write down the numerical information.}\\
                \textit{3. Write down what you are asked to find.}\\
                \textit{4. Write an equation that relates the variables.}\\
                \textit{5. Differentiate with respect to t.}\\
                \textit{6. Evaluate.}\\
        \subsection{Linearization and Differentials}
            \paragraph{Linearization} If $f$ is a differentiable at $x=a$, then the approximating function
            \[L(x)=f(a)+f'(a)(x-a)\]
            is the \textbf{linearization} of $f$ at $a$. The approximation
            \[f(x)\approx L(x)\]
            of $f$ by $L$ is the \textbf{standard linear approximation} of $f$ at $a$. The point $x=a$ is the \textbf{center} of the approximation.
            \paragraph{Differentials} Let $y=f(x)$ be a differentiable function. The \textbf{differential} $\bm{dx}$ is an independent variable. The \textbf{differential} $\bm{dy}$ is
            \[dy=f'(x)dx\]
            \paragraph{Estimating with Differentials}
            Since
            \[f(a+dx)=f(a)+\Delta y\]
            the differential approximation gives
            \[f(a+dx)\approx f(a)+dy\]
            when $dx=\Delta x$. Thus the approximation $\Delta y \approx dy$ can be used to estimate $f(a+dx)$ when $f(a)$ is known, $dx$ is small,and $dy=f'(a)dx$.
            \paragraph{Error in Differential Approximation}
            If $y=f(x)$ is differentiable at $x=a$ and $x$ changes from $a$ to $a+\Delta x$, the change $\Delta y$ in $f$ is given by 
            \[\Delta y=f'(a) \Delta x+\epsilon\Delta x\]
            in which $\epsilon\to 0$ as $\Delta x\to 0$\\

            \paragraph{Sensitivity to Change}
            \begin{tabular}{ccc}
                \hline
                &\textbf{True}&\textbf{Estimated}\\
                \hline
                Absolute change & $\Delta f=f(a+dx)-f(a)$ & $df=f'(a)dx$\\
                Relative change&$\cfrac{\Delta f}{f(a)}$&$\cfrac{df}{f(a)}$\\
                Percentage change&$\cfrac{\Delta f}{f(a)}\times 100$&$\cfrac{df}{f(a)}\times 100$\\
            \end{tabular}

    \newpage
    \section{Applications of Derivatives}
        \subsection{Extreme Values of Functions}
            \paragraph{DEFINITIONS} Let $f$ be a function with domain $D$. Then $f$ has an \textbf{absolute maximum} value on $D$ at a point $c$ if
            \[f(x)\ge f(c)\quad \text{for all $x$ in $D$}\]
            and an \textbf{absolute minimum} value on $D$ at $c$ if
            \[f(x)\ge f(c)\quad \text{for all $x$ in $D$}\]
            which are called \textbf{extreme values} of the function $f$.
            \paragraph{Local(Relative) Extreme Values} A function $f$ has a $local maximum$ value at a point $c$ within its domain $D$ if $f(x) \le f(c)$ for all $x\in D$ lying in some open interval containing $c$.\\ \\
            A function $f$ has a \textbf{local minimum} value at a point $c$ within its domain $D$ if $f(x) \ge f(c)$ for all $x\in D$ lying in some open interval containing $c$.

            \paragraph{Finding Extrema}
            \subparagraph{Theorem---The First Derivative Theorem for Local Extreme Values} If $f$ has a local maximum or minimum value at an interior point $c$ of its domain, and if $f'$ is defined at $c$, then
            \[f'(c)=0\]
            \subparagraph{Definition} An interior point of the domain of a function $f$ where $f'$ is zero or undefined is \textbf{critical point} of $f$
        \subsection{The Mean Value Theorem*}
            \paragraph{Rolle's Theorem} Suppose that $y=f(x)$ is continuous over the closed interval $[a,b]$ and differentiable at every point of its interior $(a,b)$. If $f(a)=f(b)$, then there is at least one number $c$ in $(a,b)$ at which $f'(c)=0$.
            \paragraph{The Mean Value Theorem*} Suppose $y=f(x)$ is continuous over a closed interval $[a,b]$ and differentiable on the interval's interior $(a,b)$. Then there is at least one point $c$ in $(a,b)$ at which
            \[\cfrac{f(b)-f(a)}{b-a}=f'(c).\]
            \paragraph{Corollary 1} If $f'(x)=0$ at each point $x$ of an open interval $(a,b)$, then $f(x)=C$ for all $x\in (a,b)$, where $C$ is a constant.
            \paragraph{Corollary 2} If $f'(x)=g'(x)$ at each point $x$ in an open interval $(a,b)$, then there exists a constant $C$ such that $f(x)=g(x)+C$ for all $x\in (a,b)$. That is, $f-g$ is a constant function on $(a,b)$. 
        \subsection{Monotonic Functions and the First Derivative Test}
            \paragraph{Corollary 3} Suppose that $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$.\\
                If $f'(x) > 0$ at each point $x\in (a,b)$, then $f$ is increasing on $[a,b]$.\\
                If $f'(x) < 0$ at each point $x\in (a,b)$, then $f$ is decreasing on $[a,b]$.\\
        \subsection{Concavity and Curve Sketching}
            \paragraph{Concavity} The graph of a differentiable function $y=f(x)$ is\\
            (a) \textbf{concave up} on an open interval $I$ if $f'$ is increasing on $I$.\\
            (b) \textbf{concave down} on an open interval $I$ if $f'$ is decreasing on $I$.

            \paragraph{The Second Derivative Test for Concavity} 
            Let $y=f(x)$ be twice-differentiable on an interval $I$.\\
            1. If $f''>0$ on $I$, the graph of $f$ over $I$ is concave up.\\
            2. If $f''<0$ on $I$,the graph of $f$ over $I$ is concave down.\\
            
            \paragraph{Points of Inflection}
                \subparagraph{Definition} A point $(c,f(c))$ where the graph of a function has a tangent line and where the Concavity changes is a \textbf{point of inflection}.
            \newline\\
            *At a point of inflection $(c,f(c))$, either $f''(c)=0$ or $f''y$\\
            \paragraph{Second Derivative Test for local Extrema}
            Suppose $f''$ is continuous on an open interval that contains $x=c$.\\
            1. If $f'(c)=0$ and $f''(c)<0$, then $f$ has a local maximum at $x=c$.\\
            2. If $f'(c)=0$ and $f''(c)>0$, then $f$ has a local minimum at $x=c$.\\
            3. If $f'(c)=0$ and $f''(c)=0$, then the test fails. The function $f$ may have a local maximum, a local minimum, or neither.\\

        \subsection{Applied Optimization}
            \textbf{Solving Applied Optimization Problems}\\
            1. \textit{Read the problem.}\\
            2. \textit{Draw a picture.}\\
            3. \textit{Introduce variables.}\\
            4. \textit{Write an equation for the unknown quantity.}\\
            5. \textit{Test the critical points and endpoints in the domain of the unknown.}\\
        
        \subsection{Newton's Method}
            \textbf{Newton's Method}\\
            1. Guess a first approximation to a solution of the equation $f(x)=0$. A graph of $y=f(x)$ may help.
            2. Use the first approximation to get a second, the second to get a third, and so on, using the formula
            \[x_{n+1}=x_n -\cfrac{f(x_n)}{f'(x_n)},\quad \text{if $f'(x_n)\ne 0$}.\]

        \subsection{Antiderivative}
            \paragraph{DEFINITION} A function $F$ is an \textbf{antiderivative} of $f$ on an interval $l$ if $F'(x)=f(x)$ for all $x$ in $I$.
            \paragraph{THEOREM} If $F$ is an antiderivative of $f$ on an interval $I$, then the most general antiderivative of $f$ on $I$ is
            \[F(x)+C\]
            where $C$ is an arbitrary constant.
            \\\\
            \textbf{Antiderivative formulas, $k$ a nonzero constant}\\
            \begin{tabular}{|ll|}
                \hline
                \textbf{Function}&\textbf{General antiderivative}\\
                \hline
                1. $x^n$&$\cfrac{1}{n+1}x^{n+1}+C,\quad n\ne -1$\\
                2. $\sin kx$&$-\cfrac{1}{k}\cos kx+C$\\
                3. $\cos kx$&$\cfrac{1}{k}\sin kx+C$\\
                4. $\sec ^2 kx$&$\cfrac{1}{k}\tan kx+C$\\
                5. $\csc ^2 kx$&$-\cfrac{1}{k}\cot kx+C$\\
                6. $\sec kx \tan kx$&$\cfrac{1}{k}\sec kx+C$\\
                7. $\csc kx \cot kx$&$-\cfrac{1}{k}\csc kx+C$\\&\\
                \hline
            \end{tabular}
            \\\\
            \textbf{Antiderivative linearity rules}\\
            \begin{tabular}{|lll|}
                \hline
                &\textbf{Function}&\textbf{General antiderivative}\\
                \hline956
                1.  \textit{Constant Multiple Rule:}&$kf(x)$&$kF(x)+C$, $k$ a constant\\
                2.  \textit{Negative Rule:}&$-f(x)$&$-F(x)+C$\\
                3.  \textit{Sum or Difference Rule:}&$f(x)\pm g(x)$&$F(x)\pm G(x)+C$\\
                \hline
            \end{tabular}
            \paragraph{Indefinite Integrals}
                \subparagraph{DEFINITION} The collection of all antiderivatives of $f$ is called the \textbf{indefinite integral} of $f$ with respect to $x$, and is denoted by
                \[\int f(x)\; dx\]
                The symbol $\int$ is an \textbf{integral sign}. The function $f$ is the integrand of the integral, and $x$ is the \textbf{variable of integration}.
            \paragraph{examples}
            \[\int 2x\; dx=x^2+C\]
            \[\int \cos x \; dx=\sin x+C\]
            \[\int (\sec ^2 x+\cfrac{1}{x\sqrt{x}})\; dx=\tan x+\sqrt{x}+C\]

    \newpage
    \section{Integrals}
        \subsection{Area and Estimating with Finite Sums}
            \[\text{SUM}=\sum_{i=1}^{n}f(c_{i})\Delta x\]
        \subsection{Sigma Notation and Limits of Finite Sums}
            \textbf{Finite Sums and Sigma Notation}\\\\
            \textbf{Sigma notation} enables us to write a sum with many terms in the compact form
            \[\textbf{Sigma Notation}:\quad\sum_{k=1}^{n}a_{k}=a_1+a_2+a_3+\cdots +a_{n-1}+a_{n}\]
            \textbf{Algebra Rules for Finite Sums}\\
            \begin{tabular}{|ll|}
                \hline
                1. \textit{Sum Rule:}&$\sum_{k=1}^{n}(a_k+b_k)+\sum_{k=1}^{n}a_k+\sum_{k=1}^{n}b_k$\\
                2. \textit{Difference Rule:}&$\sum_{k=1}^n(a_k-b_K)=\sum_{k=1}^{n}a_k-\sum_{k=1}^{n}b_k$\\
                3. \textit{Constant Multiple Rule:}&$\sum_{k=1}^{n}ca_k=c\cdot \sum_{k=1}^{n}a_k$\\
                4. \textit{Constant Value Rule:}&$\sum_{k=1}^{n}c=n\cdot c$\\
                \hline
            \end{tabular}
            \\
            \begin{tabular}{|ll|}
                \hline
                The first $n$ squares:&$\sum_{k=1}^{n}k^2=\cfrac{n(n+1)(2n+1)}{6}$\\
                The first $n$ cubes:&$\sum_{k=1}^{n}k^3=(\cfrac{n(n+1)}{2})^2$\\
                \hline
            \end{tabular}
            \paragraph{Riemann Sums}
                \subparagraph{Riemann sum for $f$ on the interval $\bf{[a,b]}$.}
                \[S_n=\sum_{k=1}^{n}f(c_k)\Delta x_k=\sum_{k=1}^{n}f(a+k\cfrac{(b-a)}{n})\cdot (\cfrac{b-a}{n})\]

        \subsection{The Definite Integral}
            \paragraph{Definition of the Definite Integral}
                Let $f(x)$ be a function defined on a closed interval $[a,b]$. We say that a number $J$ is the \textbf{definite integral of $f$ over $[a,b]$} and that $j$ is the limit of the Riemann sums $\sum_{k=1}^{n}f(c_k)\Delta x_k$ if the following condition is satisfied.\\
                Given any number $\epsilon > 0$ there is a corresponding number $\delta > 0$ such that for every partition $P={x_0,x_1,\cdots,x_n}$ of $[a,b]$ with $\Vert P \Vert < \delta$ and any choice of $c_k$ in $[x_{k-1},x_k]$, we have
                \[\lvert \sum_{k=1}^{n}f(c_k)\delta x_k -J\rvert<\epsilon\]
                \[J=\lim\limits_{\Vert P\Vert \to 0}\sum_{k=1}^{n}f(c_k)\Delta x_k\]
                \[\textbf{Symbol:}\quad\int_{a}^{b}f(x)dx\]
                named \textbf{``Integral of $f$ from $a$ to $b$''}.
                \[\int_{a}^{b}f(x)dx=\lim\limits_{n\to \infty}\sum_{k=1}^{n}f(a+k\cfrac{(b-a)}{n})\cdot (\cfrac{b-a}{n})\]
            \paragraph{Integrable and Nonintegrable Functions}
                \subparagraph{Integrability of Continuous Functions} If a function $f$ is continuous over the interval $[a,b]$, or if $f$ has at most finitely many jump discontinuities there, then the definite integral $\int_{a}^{b} f(x)dx$ exists and $f$ is integrable over $[a,b]$.
            \paragraph{Properties of Definite Integrals}
                \[\int_b^af(x)dx=-\int_a^bf(x)dx\]
                \[\int_a^af(x)dx=0\]
                \subparagraph{Theorem} When $f$ and $g$ are integrable over the interval $[a,b]$, the definite integral satisfies the rules below:\\\\
                \textbf{Rules satisfied by definite integrals}\\
                \begin{tabular}{|ll|}
                    \hline
                    1. \textit{Order of Integration:}&$\int_b^af(x)dx=-\int_a^bf(x)dx$\\
                    2. \textit{Zero Width Interval:}&$\int_a^af(x)dx=0$\\
                    3. \textit{Constant Multiple:}&$\int_a^bkf(x)dx=k\int_a^bf(x)dx$\\
                    4. \textit{Sum and Difference:}&$\int_a^b(f(x)\pm g(x))dx=\int_a^bf(x)dx\pm \int_a^bg(x)dx$\\
                    5. \textit{Additivity:}&$\int_a^bf(x)dx+\int_b^cf(x)dx=\int_a^cf(x)dx$\\
                    6. \textit{Max-Min Inequality:}&If $f$ has maximum value max $f$ and minimum\\& value min $f$ on $[a,b]$, then\\&$\text{min}f\cdot(b-a)\le \int_a^bf(x)dx\le \text{max}f\cdot(b-a)$\\
                    7. \textit{Domination:}&$f(x)\ge g(x)\ \text{on}\ [a,b] \Rightarrow \int_a^bf(x)dx\le \int_a^b g(x)dx$\\&$f(x)\ge 0\ \text{on}\ \Rightarrow\int_a^bf(x)dx\ge 0$\\
                    \hline
                \end{tabular}
            \paragraph{Area Under the Graph of a Nonnegative Funcion}
                \subparagraph{Definition} If $y=f(x)$ is nonnegative and integrable over a closed interval $[a,b]$, then the \textbf{area under the curve $\bf{y=f(x)}$ over $\bf{[a,b]}$} is the integral of $f$ from $a$ to $b$.
                \[A=\int_a^bf(x)dx\]
                We have the following rules:
                \begin{equation}
                    \begin{aligned}
                        \int_a^bxdx&=\cfrac{b^2}{2}-\cfrac{a^2}{2},\quad a<b\\
                        \int_a^bcdx &=c(b-a),\quad c\text{ any constant}\\
                        \int_a^bx^2dx&=\cfrac{b^3}{3}-\cfrac{a^3}{3},\quad a<b\\
                    \end{aligned}
                \end{equation}
            \paragraph{Average Value of a Continuous Function Revisited}
                \[\text{av}(f)=\cfrac{1}{b-a}\int_a^bf(x)dx\]
                is $f$'s \textbf{average value on $\bf{[a,b]}$}, also called it's \textbf{mean}.
        \subsection{The Fundamental Theorem of Calculus}
            \paragraph{Mean Value Theorem for Definite Integrals} If $f$ is continuous on $[a,b]$, then at some point $c$ in $[a,b]$,
            \[f(c)=\cfrac{1}{b-a}\int_a^b f(x)dx\]
            \paragraph{Fundamental Theorem, Part 1}
            \text{}\\
            \par If $f(t)$ is an integrable function over a finite interval $I$, then the integral from any fixed number $a\in I$ to another number $x\in I$ defines a new function $F$ whose value at $x$ is
            \[F(x)=\int_a^xf(t)dt\]
            \par For example, if $f$ is nonnegative and $x$ lies to the right of $a$, then $F(x)$ is the area under the graph from $a$ to $x$. The variable x is the upper limit of integration of an integral, but $F$ is just like any other real-valued function of a real variable. For each value of the input $x$, there is a well-defined numerical output, in this case the definite integral of $f$ from $a$ to $x$.
            \par This equation gives a way to define new functions, but its importance now is the connection it makes between integrals and derivatives. If $f$ is any continuous function, then the Fundamental Theorem asserts that $F$ is a differentiable function of $x$ whose derivative is $f$ itself. At every value of $x$, it asserts that
            \[\cfrac{d}{dx}F(x)=f(x).\]
            \par To gain some insight into why this result holds, we look at the geometry behind it.
            \par If $f\ge 0$ on $[a,b]$, then the computation of $F'(x)$ from the definition of teh derivative means taking the limit as $h\to 0$ of the difference quotient
            \[\cfrac{F(x+h)-F(x)}{h}\]
            \par For $h>0$, the numerator is obtained by subtracting two areas, so it is the area under the graph of $f$ from $x$ to $x+h$. If $h$ is small, this area is approximately equal to the area of the rectangle of height $f(x)$ and width $h$. That is
            \[F(x+h)-F(x)\approx hf(x)\]
            \par Dividing both sides of this approximation by $h$ and letting $h\to 0$, it is reasonable to expect that
            \[F'(x)=\lim\limits_{h\to 0}\cfrac{F(x+h)-F(x)}{h}=f(x)\]
            \par This result is true even if the function $f$ is not positive, and it forms the first part of the Fundamental Theorem of Calculus.
            \textbf{EXAMPLE:Find $dy/dx$ if}\\
            \[y=\int_{1+3x^2}^4 \cfrac{1}{2+t}dt\]
            \[\text{let}\ u=1+3x^2\]
            \begin{equation}
                \begin{aligned}
                    \cfrac{dy}{dx}&=\cfrac{dy}{du}\cdot\cfrac{du}{dx}\\
                    &=\cfrac{d}{du}\int_u^4 \cfrac{1}{2+t}dt\cdot(6x)\\
                    &=-\cfrac{d}{du}\int_4^u\cfrac{1}{2+t}dt\cdot(6x)\\
                    &=-\cfrac{1}{2+u}\cdot(6x)\\
                    &=-\cfrac{2x}{x^2+1}
                \end{aligned}
            \end{equation}
            \paragraph{Proof of Theorem}
            \begin{equation}
                \begin{aligned}
                    F'(x)&=\lim\limits_{h\to 0}\cfrac{F(x+h)-F(x)}{h}\\
                    &=\lim\limits_{h\to 0}\cfrac{1}{h}[\int_0^{x+h}f(t)dt-\int_a^x f(t)dt]\\
                    &=\lim\limits_{h\to 0}\cfrac{1}{h}\int_x^{x+h}f(t)dt\\
                \end{aligned}
            \end{equation}
            \par According to the Mean Value Theorem for Definite Integrals, the value before taking the limit in the last expression is one of the values taken on by $f$ in the interval between $x$ and $x+h$. That is, for some numer $c$ in this interval,\\
            \begin{equation}
                \cfrac{1}{h}\int_x^{x+h}f(t)dt=f(c)
            \end{equation}
            \par As $h\to 0$, $x+h$ approaches $x$, forcing $c$ to approach $x$ also (because $c$ is trapped between $x$ and $x+h$). Since $f$ is continuous at $x$, f(c) approaches $f(x)$
            \begin{equation}
                \lim\limits_{h\to 0}f(c)=f(x)
            \end{equation}
            \par In conclusion, we have
            \begin{equation}
                \begin{aligned}
                    F'(x)&=\lim\limits_{h\to 0}\cfrac{1}{h}\int_x^{x+h}f(t)dt\\
                    &=\lim\limits_{h\to 0}f(c)\\
                    &=f(x)\\
                \end{aligned}
            \end{equation}
            \par If $x=a$ or $b$, then the limit of Equation is interpreted as a one-sided limit with $h\to 0^+$ or $h\to 0^-$, respectively.
            \paragraph{Fundamental Theorem, Part 2(The Evaluation Theorem)}
                \subparagraph{The Fundamental Theorem of Calculus, Part 2}
                If $f$ is continuous over $[a,b]$ and $F$ is any antiderivative of $f$ on $[a,b]$, then
                \[\int_a^b f(x)dx=F(b)-F(a)=F(x)\Bigg]_a^b=\Bigg[F(x)\Bigg]_a^b\]
                \subparagraph{Example:}
                \begin{equation}
                    \begin{aligned}
                        \int_1^4(\cfrac{3}{2}\sqrt{x}-\cfrac{4}{x^2})dx&=\Bigg[x^{3/2}+\cfrac{4}{x}\Bigg]_1^4\\
                        &=[8+1]-[5]\\
                        &=4
                    \end{aligned}
                \end{equation}
            \paragraph{The Integral of a Rate}
            \[\int_a^bF'(x)dx=F(b)-F(a)\]
            \[F(b)=F(a)+\int_a^b F'(x)dx\]
            \paragraph{The Relationship Between Integration and Differrentiation}
            \[\cfrac{d}{dx}\int_a^xf(t)dt=f(x)\]
        \subsection{Indefinite Integrals and the Substitution Method}
            The indefinite integral $\int$ notation  means for any antiderivative $F$ of $f$,
            \[\int f(x)dx=F(x)+C\]
            where $C$ is an arbitrary constant.
            \paragraph{Substitution: Running the Chain Rule Backwards}
                If $u$ is a differentiable function of $x$ and $n$ is an number different form -1, the Chain Rule tells us that 
                \[\cfrac{d}{dx}(\cfrac{u^{n+1}}{n+1})=u^n\cfrac{du}{dx}\]
                From another point of view, this same equation says that $u^{n+1}/(n+1)$ is one of the antiderivatives of the function $u^n(du/dx)$. Therefore,
                \[\int u^n\cfrac{du}{dx}dx=\cfrac{u^{n+1}}{n+1}+C\]
                The integral in the equation is equal to the simpler integral
                \[\int u^n du=\cfrac{u^{n+1}}{n+1}+C\]
                \[du=\cfrac{du}{dx}dx\]
            \paragraph{Example 1} Find the integral $\int (x^3+x)^5 (3x^2+1)dx$
                \subparagraph{Solution} We set $u=x^3+x$. Then
                \[du=\cfrac{du}{dx}dx=(3x^2+1)dx\]
                so that by Substitution we have
                \begin{equation}
                    \begin{aligned}
                        \int (x^3+x)^5(3x^2+1)dx&=\int u^5 du\\
                        &=\cfrac{u^6}{6}+C\\
                        &=\cfrac{(x^3+x)^6}{6}+C\\
                    \end{aligned}
                \end{equation}
            \paragraph{Example 2} Find $\int \sqrt{2x+1}dx$
                \subparagraph{Solution} 
                \begin{equation}
                    \begin{aligned}
                        \int \sqrt{2x+1}dx&=\cfrac{1}{2}\int \sqrt{2x+1}\cdot 2dx\\
                        &=\cfrac{1}{2}\int u^{\cfrac{1}{2}}du\\
                        &=\cfrac{1}{2}\cfrac{u^{3/2}}{3/2}+C\\
                        &=\cfrac{1}{2}(2x+1)^{\frac{3}{2}}+C
                    \end{aligned}
                \end{equation}
            \paragraph{The Substitution Rule} If $u=g(x)$ is a differentiable function whose range is an interval $I$, and $f$ is continuous on $I$, then
            \[\int f(g(x))g'(x)dx=\int f(u)du\]
            \paragraph{Example 3} Find $\int \sec^2(5x+1)\cdot 5dx$
                \subparagraph{Solution} We substitute $u=5x+1$ and $du=5dx$. Then
                \begin{equation}
                    \begin{aligned}
                        \int \sec^2(5x+1)\cdot 5 dx&=\int \sec ^2u\ du\\
                        &=\tan u+C\\
                        &=\tan(5x+1)+C
                    \end{aligned}
                \end{equation}
        \subsection{Definite Integral Substitutions and the Area Between Curves}
            \paragraph{The Substitution Formula}
                \subparagraph{Substitution in Definite Integrals} If $g'$ is continuous on the interval $[a,b]$ and $f$ is continuous on the range of $g(x)=u$, then
                \[\int_a^bf(g(x))\cdot g'(x)dx=\int_g(a)^g(b)f(u)du\]
                \subparagraph{Proof} Let $F$ denote any antiderivative of $f$. Then,
                \begin{equation}
                    \begin{aligned}
                        \int_a^bf(g(x))\cdot g'(x)dx&=F(g(x))\Bigg ]_{x=a}^{x=b}\\
                        &=F(g(b))-F(g(a))\\
                        &=F(u)\Bigg ]_{u=g(a)}^{u=g(b)}\\
                        &=\int_{g(a)}^{g(b)}f(u)du\\
                    \end{aligned}
                \end{equation}
            \paragraph{Example 1} Evaluate $\int_{-1}^1 3x^2\sqrt{x^3+1}dx$
                \subparagraph{Solution}
                \begin{equation}
                    \begin{aligned}
                        \int_{-1}^1 3x^2\sqrt{x^3+1}dx&=\int_0^2\sqrt{u}du\\
                        &=\cfrac{2}{3}u^{3/2}\Bigg ]_0^2\\
                        &=\cfrac{2}{3}[2^{3/2}-0^{3/2}]\\
                        &=\cfrac{2}{3}[2\sqrt{2}]\\
                        &=\cfrac{4\sqrt{2}}{3}
                    \end{aligned}
                \end{equation}\\
            \paragraph{Definite Integrals of Symmetric Functions}
                \subparagraph{Theorem} Let $f$ be continuous on the symmetric interval $[-a,a]$.\\
                (a) If $f$ is even, then $\int_{-a}^af(x)dx=2\int_0^af(x)dx$\\
                (b) If $f$ is odd, then $\int_{-a}^af(x)dx=0$
            \paragraph{Areas Between Curves}
                \subparagraph{Definition} If $f$ and $g$ are continuous with $f(x)\ge g(x)$ throughout $[a,b]$, then the \textbf{area of the region between the curves $\bf{y=f(x)}$ and $\bf{y=g(x)}$ from $\bf(a)$ to $\bf{b}$} is the integral of $(f-g)$ from $a$ to $b$:
                \[A=\int_a^b[f(x)-g(x)]dx\]
            \paragraph{Integration with Respect to $y$}
                \[A=\int_c^d [f(y)-g(y)]dy\]
    
    \newpage
    \section{Applications of Definite Integrals}
        \subsection{Volumes Using Cross-Sections}
            \paragraph{Definition} The \textbf{volume} of a solid of integrable cross-sectional area $A(x)$ from $x=a$ to $x=b$ is the integral of $A$ from $a$ to $b$.
            \[V=\int_a^b A(x)dx\]
                \subparagraph{Calculating the Volume of a Solid}
                \text{}\\
                1. \textit{Sketch the solid and a typical cross-section.}\\
                2. \textit{Find a formula for $A(x)$, the area of a typical cross-section.}\\
                3. \textit{Find the limits of integration.}\\
                4. \textit{Integrate $A(x)$ to find the volume.}
            \paragraph{Solid of Revolution: The Disk Method} The solid generated by rotating (or revolving) a plane region about an axis in its plane is called a \textbf{solid of revolution}.
            \[A(x)=\pi (\text{radius})^2=\pi [R(x)]^2\]
                \subparagraph{Volume by Disks for Rotation About the $x$-axis}
                \[V=\int_a^b A(x)\ dx=\int_a^b\pi [R(x)]^2\ dx\]
            \paragraph{Example} Find the volume of the solid generated by revolving the region bounded by $y=\sqrt{x}$ and the lines $y=1$, $x=4$ about the line $y=1$.
                \subparagraph{Solution}
                \begin{equation}
                    \begin{aligned}
                        V&=\int_1^4 \pi[R(x)]^2\ dx\\
                        &=\int_1^4 \pi[\sqrt{x}-1]^2\ dx\\
                        &=\pi \int_1^4[x-2\sqrt{x}+1] dx\\
                        &=\pi \Bigg [\cfrac{x^2}{2}-2\cdot\cfrac{2}{3}x^{3/2}+x\Bigg]_1^4\\
                        &=\cfrac{7\pi}{6}
                    \end{aligned}
                \end{equation}
                \subparagraph{Volume by Disks for Rotation About the $y$-axis}
                \[V=\int_c^d A(y)\ dy=\int_c^d\pi [R(y)]^2\ dy\]
            \paragraph{Solids of Revolution: The Washer Method}
                \subparagraph{Volume by Washers for Rotation About the $x$-axis}
                \[V=\int_a^b A(x)\ dx=\int_a^b \pi([R(x)]^2-[r(x)]^2)dx\]
        \subsection{Volumes Using Cylindrical Shells}
            \paragraph{Slicing with Cylinders}
            Unrolling a cylindrical shell shows that its volume is approximately that of a rectangular slab with area $A(x)$ and thickness $\Delta x$.
            \paragraph{The Shell Method}
            \begin{equation}
                \begin{aligned}
                    \Delta V_k&=2\pi \times \text{average shell radius} \times \text{shell height} \times \text{thickness}\\
                    &=2\pi \cdot(c_k-L) \cdot f(c_k)\cdot \Delta x_k\\
                    V&=\lim\limits_{n\to \infty} \sum_{k=1}^n \Delta V_k\\
                    &=\int_a^b 2\pi \text{(shell radius)(shell height)} dx\\
                    &=\int_a^b 2\pi (x-L)f(x)\ dx
                \end{aligned}
            \end{equation}
        \subsection{Arc Length}
            \paragraph{Length of a Curve $y=f(x)$}
                Suppose the curve whose length we want to find is the graph of the function $y=f(x)$ from $x=a$ to $x=b$. In order to derive an integral formula for the length of the curve, we assume that $f$ has a continuous derivative at every point of every point of $[a,b]$. Such a function is called \textbf{smooth}, and its graph is a \textbf{smooth curve} because it does not have any breaks, corners, or cusps.
                \[L_k=\sqrt{(\Delta x_k)^2+(\Delta y_k)^2}\]
                \[\sum_{k=1}^n L_k=\sum_{k=1}^n \sqrt{(\Delta x_k)^2+(\Delta y_k)^2}\]
                \[\Delta y_k=f'(c_k)\Delta x_k\]
                \[\sum_{k=1}^n L_k=\sum_{k=1}^n \sqrt{(\Delta x_k)^2+(f'(c_k)\Delta x_k)^2}=\sum_{k=1}^n \sqrt{1+[f'(c_k)]^2}\Delta x_k\]
                \[\lim\limits_{n\to \infty}\sum_{k=1}^n L_k=\lim\limits_{n\to \infty}\sum_{k=1}^n \sqrt{1+[f'(c_k)]^2}\Delta x_k=\int_a^b \sqrt{1+[f'(x)]^2}dx\]
                \[L=\int_a^b \sqrt{1+[f'(x)]^2}dx=\int_a^b \sqrt{1+(\cfrac{dy}{dx})^2}dx\]
            \paragraph{Dealing with Discontinuities in $dy/dx$}
                \subparagraph{Formula for the Length of $x=g(y),\ c\le y \le d$}
                \[L=\int_c^d \sqrt{1+[g'(y)]^2}dy=\int_c^d \sqrt{1+(\cfrac{dx}{dy})^2}dy\]
            \paragraph{The Differential Formula for Arc Length}
                \text{}\\
                \par If $y=f(x)$ and if $f'$ is continuous on $[a,b]$, then by the Fundamental Theorem of Calculus we can define a new function
                \[s(x)=\int_a^x \sqrt{1+[f'(t)]^2}\ dt\]
                \[\cfrac{ds}{dx}=\sqrt{1+[f'(x)]^2}=\sqrt{1+\bigg (\cfrac{dy}{dx}\bigg )^2}\]
                \[ds=\sqrt{1+\bigg(\cfrac{dy}{dx}\bigg)^2}\ dx\]
                \[ds=\sqrt{dx^2+dy^2}\]
        \subsection{Areas of Surfaces of Revolution}
            \paragraph{Defining Surface Area}
            \begin{equation}
                \begin{aligned}
                    \text{Frustum surface area}&=2\pi \cdot \cfrac{f(x_{k-1})+f(x_k)}{2}\cdot \sqrt{(\Delta x_k)^2+(\Delta y_k)^2}\\
                    &=\pi (f(x_{k-1})+f(x_k))\sqrt{(\Delta x_k)^2+(\Delta y_k)^2}
                \end{aligned}
            \end{equation}
                \subparagraph{Definition} If the function $f(x)\ge 0$ is continuously differentiable on $[a,b]$, the \textbf{area of the surface} generated by revolving the graph of $y=f(x)$ about the $x$-axis is  
                \[S=\int_a^b 2\pi y\sqrt{1+\bigg( \cfrac{dy}{dx} \bigg)^2} dx=\int_a^b 2\pi f(x) \sqrt{1+(f'(x))^2}\ dx\]
            \paragraph{Revolution About the $y$-Axis}
                \[S=\int_c^d 2\pi x\sqrt{1+\bigg( \cfrac{dx}{dy} \bigg)^2} dy=\int_c^d 2\pi g(y) \sqrt{1+(g'(x))^2}\ dy\]
        \subsection{Work and Wfuid Forces}
            \paragraph{Work Done by a Constant Force}
            \[W=Fd\qquad\text{(Constant-force formula for work.)}\]
            \paragraph{Work Done by a Variable Force Along a Line}
            \[W=\int_a^bF(x)dx\]
            \paragraph{Hooke's Law for Springs:$F=kx$}
                \subparagraph{Hooke's Law} The force required to hold a stretched or compressed spring $x$ units from its nature (unstressed) length is proportional to $x$. In symbol
                \[F=kx\]
            \paragraph{Lifting Objects and Pumping Liquidsfrom Containers}
            \paragraph{Fluid Pressure and Forces}    
                \subparagraph{The Pressure-Depth Equation}
                \[p==wh\]
                \[F=pA=whA\]
                \subparagraph{The Integral for Fluid Force Against a Vertical Flat Plate}
                \[F=\int_a^b w\cdot(\text{strip depth})\cdot L(y)\ dy\]
        \subsection{Moments and Centers of Mass}
            \paragraph{Masses Along a Line}
            \[\textbf{System Torque}=\sum_{k=1}^n m_k g x_k\]
            \[M_0=\text{Moment of system about origin}=\sum_{k=1}^n m_k x_k\]
            \begin{equation}
                \begin{aligned}
                    \sum (x_k-\overline{x})m_k g&=0\\
                    \overline{x}&=\cfrac{\sum m_k x_k}{\sum m_k}\\
                    &=\cfrac{\text{System moment about origin}}{\text{System mass}}
                \end{aligned}
            \end{equation}
            \paragraph{Masses Distributed over a Plane Region}
            \text{}\\
            \par System mass:   $M=\sum m_k$\\
            \par Moment about $x$-axis:     $M_x=\sum m_k y_k$\\
            \par Moment about $y$-axis:     $M_y=\sum m_k x_k$\\
            \[\overline{x}=\cfrac{M_y}{M}=\cfrac{\sum m_k x_k}{\sum m_k}\]
            \[\overline{y}=\cfrac{M_x}{M}=\cfrac{\sum m_k y_k}{\sum m_k}\]
            \paragraph{Thin, Flat Plates}
                \subparagraph{Moments, Mass and Center of Mass of a Thin Plate Covering a Region in the $xy$-Plane}
                \text{}\\ \par
                \begin{tabular}{|rl|}
                    \hline
                    Moment about the $x$-axis:&\quad $M_x=\int \widetilde{y} dm$\\
                    Moment about the $y$-axis:&\quad $M_y=\int \widetilde{x} dm$\\
                    Mass:&\quad $M=\int dm$\\
                    Center of mass:&\quad $\overline{x}=\cfrac{M_y}{M}$,\ $\overline{y}=\cfrac{M_x}{M}$\\
                    \hline
                \end{tabular}
            \paragraph{Plates Bounded by Two Curves}
                \[\overline{x}=\cfrac{1}{M}\int_a^b \delta x[f(x)-g(x)]dx\]
                \[\overline{y}=\cfrac{1}{M}\int_a^b \frac{\delta}{2}[f^2(x)-g^2(x)]dx\]
            \paragraph{Centroids}
            \paragraph{Fluid Forces and Centroids}
            \[F=w\overline{h}A\]
            \paragraph{The Theorems of Pappus}
                \subparagraph{Pappus's Theorem for Volumes}If  a  plane  region  is  revolved  once about a line in the plane that does not cut through the regions interior, then the volume of the solid it generates is equal to the regions area times the distance traveled by the regions centroid during the revolution. If $\rho$ is the distance from the axis of revolution to the centroid, then
                \subparagraph{Proof} 
                \[V=\int_c^d 2\pi (\text{shell radius})(\text{shell height})\ dy=2\pi \int_c^d yL(y) dy\]
                \[\overline{y}=\cfrac{\int_c^d \widetilde{y}\ dA}{A}=\cfrac{\int_c^d yL(y)\ dy}{A}\]
                \[\int_{c}^{d} yL(y)\ dy =A \overline{y}\]
                \subparagraph{Pappus's Theorem for Surface Areas}  If  an  arc  of  a  smooth  plane curve is revolved once about a line in the plane that does not cut through the  arcs  interior,  then  the  area  of  the  surface  generated  by  the  arc  equals  the  length $L$  of  the  arc  times  the  distance  traveled  by  the  arcs  centroid  during  the  revolution. If $\rho$ is the distance from the axis of revolution to the centroid, then
                \[S=2\pi \rho L\]
    
    \newpage
    \section{Transcendental Functions} 
        \subsection{Inverse Functions and Their Derivatives}
            \paragraph{One-to-One Functions}
            A function $f(x)$ is \textbf{One-to-One} on a domain $D$ if $f(x_1)\ne f(x_2)$ whenever $x_1\ne x_2$ in $D$.
            \paragraph{Inverse Functions}
            Suppose that $f$ is a one-to-one function on a domain $D$ with range $R$. The \textbf{Inverse function} $f^{-1}$ is defined by
            \[f^{-1}(b)=a\quad \text{if}\quad f(a)=b\] 
            The domain of $f^{-1}$ is $R$ and the range of $f^{-1}$ is $D$.
            \begin{equation}
                \begin{aligned}
                    (f^{-1}\circ f)(x)=x&,\quad \text{for all $x$ in the domain of $f$}\\
                    (f\circ f^{-1})(y)=x&,\quad \text{for all $y$ in the domain of $f^{-1}$}\\
                \end{aligned}
            \end{equation}
            \paragraph{Finding Inverses}
            \paragraph{Derivatives of Inverses of Differentiable Functions}
                \subparagraph{The Derivative Rule for Inverses} If $f$ has an interval $I$ as domain and $f'(x)$ exists and is never zero on $I$, then $f^{-1}$ is differentiable at every point in its domain (the range of $f$). The value of $(f^{-1})'$ at a point $b$ in the domain of $f^{-1}$ is the reciprocal of the value of $f'$ at the point $a=f^{-1}(b)$:
                \[(f^{-1})'(b)=\cfrac{1}{f'(f^{-1}(b))}\]
                or
                \[\cfrac{df^{-1}}{dx}\Bigg|_{x=b}=\cfrac{1}{\cfrac{df}{dx}\Bigg|_{x=f^{-1}(b)}}\]
                \subparagraph{Proof}
                \begin{equation}
                    \begin{aligned}
                        f(f^{-1}(x))&=x\\
                        \cfrac{d}{dx}f(f^{-1}(x))&=1\\
                        \cfrac{d}{dx}f^{-1}(x)\cdot f'(f^{-1}(x))&=1\\
                        \cfrac{d}{dx}f^{-1}(x)&=\cfrac{1}{f'(f^{-1}(x))}
                    \end{aligned}
                \end{equation}
        \subsection{Natural Logarithms}
            \paragraph{Definition of the Natural Logarithm Function}
                The \textbf{natural logarithm} is the function given by
                \[\ln x=\int_1^x \cfrac{1}{t}dt,\quad x>0\]
                so $\ln 1=\int_1^1 \frac{1}{t}dt=0$
                \subparagraph{Definition} The \textbf{number $\bf{e}$} is that number in the domain of the natural logarithm satisfying
                    \[\ln (e)=\int_1^e\frac{1}{t}dt=1\]
                    \[e\approx 2.71828\]
            \paragraph{The Derivative of $y=\ln x$}
                \[\cfrac{d}{dx}\ln x=\cfrac{d}{dx}\int_1^x\cfrac{1}{t}dt=\cfrac{1}{x}\]
                \[\cfrac{d}{dx}\ln x=\cfrac{1}{x}\]
                \begin{equation}
                    \cfrac{d}{dx} \ln u=\cfrac{1}{u}\cdot \cfrac{du}{dx},\quad u>0
                \end{equation}
            \paragraph{Properties of Logarithms}
                    \subparagraph{Algebraic Properties of the Nature Logarithm} For any numbers $b>0$ and $x>0$, the natural logarithm satisfies the following rules:\\\par
                    \begin{tabular}{|ll|}
                        \hline
                        1. \textit{Product Rule:}&$\ln bx=\ln b+\ln x$\\
                        2. \textit{Quotient Rule:}&$\ln \cfrac{b}{x}=\ln b-\ln x$\\
                        3. \textit{Reciprocal Rule:}&$\ln \cfrac{1}{x}=-\ln x$\\
                        4. \textit{Power Rule:}&$\ln x^r=r\ln x$\\
                        \hline
                    \end{tabular}
            \paragraph{The Graph and Range of $\ln x$}
            \paragraph{The Integral $\int(1/u)du$}
                \[\int \cfrac{1}{u}du=\ln |u|+C\]
                If $u=f(x)$, then $du=f'(x)$ and
                \[\int \cfrac{f'(x)}{f(x)}dx=\ln |f(x)|+C\]
                whenever $f(x)$ is a differentiable function that is never zero.
            \paragraph{The Integrals of $\tan x$, $\cot x$, $\sec x$, and $\csc x$}
                \begin{equation}
                    \begin{aligned}
                        \int \tan x\ dx&=\int \cfrac{\sin x}{\cos x} dx=\int \cfrac{-du}{u}\\
                        &=-\ln |u| +C=-\ln |\cos x| +C\\
                        &=\ln \cfrac{1}{|\cos x|} +C=\ln |\sec x|+C\\
                        \int \cot x\ dx&=\int \cfrac{\cos x dx}{\sin x}=\int\cfrac{du}{u}\\
                        &=\ln |u|+C=\ln |\sin x|+C=-\ln |\csc x|+C\\
                        \int \sec x\ dx&=\int \sec x \cfrac{(\sec x+\tan x)}{(\sec x+\tan x)} dx\\
                        &=\int \cfrac{\sec ^2 x+\sec x\tan x}{\sec x+\tan x}dx\\
                        &=\int \cfrac{du}{u}=\ln |u|+C\\
                        &=\ln |\sec x+\tan x|+C\\
                        \int \csc x\ dx&=\int \csc x\cfrac{(\csc x+\cot x)}{(\csc x+\cot x)} dx\\
                        &=\int\cfrac{\csc^2 x+\csc x\cot x}{\csc x+\cot x}dx\\
                        &=\int \cfrac{-du}{u}=-\ln |u|+C\\
                        &=-\ln |\csc x+\cot x|+C\\
                    \end{aligned}
                \end{equation}
            \paragraph{Logarithmic Differentiation}
            Use laws of logarithms to simplify the formulas before differentiating.
        \subsection{Exponential Function}
            \paragraph{The Inverse of $\ln x$ and the Number $e$}
                \subparagraph{Definition} For every real number $x$, we define the \textbf{natural exponential function} to be $e^x=\exp x$.
                \subparagraph{Inverse Equations for $e^x$ and $\ln x$}
                \begin{equation}
                    \begin{aligned}
                        e^{\ln x}=x&\qquad (\text{all } x>0)\\
                        \ln(e^x)=x&\qquad (\text{all } x)\\
                    \end{aligned}
                \end{equation}
            \paragraph{The Derivative and Integral of $e^x$}
            \begin{equation}
                \begin{aligned}
                    \ln (e^x)&=x\\
                    \cfrac{d}{dx}\ln (e^x)&=1\\
                    \cfrac{1}{e^x}\cdot\cfrac{d}{dx}(e^x)&=1\\
                    \cfrac{d}{dx}e^x&=e^x\\
                    \cfrac{d}{dx}e^u&=e^u\cfrac{du}{dx}\\
                    \int e^u du&=e^u+C\\
                \end{aligned}
            \end{equation}
            \[\lim\limits_{x\to -\infty} e^x=0\]
            \[\lim\limits_{x\to \infty} e^x=\infty\]
            \paragraph{Laws of Exponents}
                \[e^{x_1}\cdot e^{x_2}=e^{x_1+x_2}\]
                \[e^{-x}=\cfrac{1}{e^x}\]
                \[\cfrac{e^{x_1}}{e^{x_2}}=e^{x_1-x_2}\]
                \[(e^{x_1})^r=e^{rx_1},\quad \text{if $f$ is rational}\]
            \paragraph{The General Exponential Function $a^x$}
                \subparagraph{Definition} For any number $a>0$ and $x$, the \textbf{exponential function with base }$\bf{a}$ is
                \[a^x=e^{x\ln a}\]
            \paragraph{Proof of the Power Rule(General Version)}
                \subparagraph{Definition} For any $x>0$ and for any real number $n$,
                \[x^n=e^{n\ln x}\]
                \subparagraph{General Power Rule for Derivatives}
                \text{}\\
                For $x>0$ and any real number $n$.
                \[\cfrac{d}{dx}x^n=nx^{n-1}\]
                If $x<0$, then the formula holds whenever the derivative, $x^n$, and $x^{n-1}$ all exist.
                \subparagraph{Proof} Differentiating $x^n$ with respect to $x$ gives\\
                \begin{equation}
                    \begin{aligned}
                        \cfrac{d}{dx}x^n&=\cfrac{d}{dx}e^{n\ln x}\\
                        &=e^{n\ln x}\cdot\cfrac{d}{dx}(n\ln x)\\
                        &=x^n\cdot \cfrac{n}{x}\\
                        &=nx^{n-1}
                    \end{aligned}
                \end{equation}
                \subparagraph{Example} Differentiate $f(x)=x^x,x>0$
                \begin{equation}
                    \begin{aligned}
                        f'(x)&=\cfrac{d}{dx}(e^{x\ln x})\\
                        &=e^{x\ln x}\cfrac{d}{dx}(x\ln x)\\
                        &=e^{x\ln x}\cdot(\ln x+x\cdot\cfrac{1}{x})\\
                        &=x^x(\ln x+1)
                    \end{aligned}
                \end{equation}
            \paragraph{The Number $e$ Expressed as a Limit}
                \subparagraph{The Number $e$ as a Limit}
                    \[e=\lim\limits_{x\to 0}(1+x)^{1/x}\]
                \subparagraph{Proof} If $f(x)=\ln x$, then $f'(x)=1/x$, so $f'(1)=1$. But, by the definition of derivative.
                    \begin{equation}
                        \begin{aligned}
                            f'(1)&=\lim\limits_{h\to 0}\cfrac{f(1+h)-f(1)}{h}\\
                            &=\lim\limits_{x\to 0}\cfrac{f(1+x)-f(1)}{x}\\
                            &=\lim\limits_{x\to 0}\cfrac{\ln (1+x)-\ln 1}{x}\\
                            &=\lim\limits_{x\to 0}\cfrac{1}{x}\ln (1+x)\\
                            &=\lim\limits_{x\to 0}\ln(1+x)^{1/x}\\
                            &=\ln \bigg[\lim\limits_{x\to 0} (1+x)^{1/x}\bigg]=1\\
                        \end{aligned}
                    \end{equation}
                    \[\lim\limits_{x\to 0}(1+x)^{1/x}=e\]
            \paragraph{The Derivative of $a^u$}
                \[\cfrac{d}{dx}a^x=\cfrac{d}{dx}e^{x\ln a}=e^{x\ln a}\cdot\cfrac{d}{dx}(x\ln a)=a^x\ln a\]
                \[\cfrac{d}{dx}a^u=a^u \ln a\cfrac{du}{dx}\]
                \[\int a^u du=\cfrac{a^u}{\ln a}+C\]
                \[\cfrac{d^2}{dx^2}(a^x)=\cfrac{d}{dx}(a^x\ln a)=(\ln a)^2 a^x\]
            \paragraph{Logarithms with Base $a$}
                \subparagraph{Definition} For any positive number $a\ne 1$, $\log_a x$ is the inverse function of $a^x$.
                \[a^{\log_a x}=x\quad(x>0)\]
                \[log_a(a^x)=x\quad (\text{all } x)\]
                \[\log_a x=\cfrac{\ln x}{\ln a}\]
                \[\ln xy=\ln x+\ln y\]
                \[\cfrac{\ln xy}{\ln a}=\cfrac{\ln x}{\ln a}+\cfrac{\ln y}{\ln a}\]
                \[\log_a xy=\log_a x+\log_a y\]
            \paragraph{Derivatives and Integrals Involving $log_a x$}
                \[\cfrac{d}{dx}(\log_a u)=\cfrac{d}{dx}\bigg(\cfrac{\ln u}{\ln a}\bigg)=\cfrac{1}{\ln a}\cfrac{d}{dx}(\ln u)=\cfrac{1}{\ln a}\cdot\cfrac{1}{u}\cfrac{du}{dx}\]
        \subsection{Exponential Change and Separable Differential Equations}
            \paragraph{Exponential Change}
                \[\cfrac{dy}{dt}=ky,\qquad y(0)=y_0\]
                \[y=y_0 e^{kt}\]
            \paragraph{Separable Differential Equations}
                \[\cfrac{dy}{dx}=f(x,y)\]
                \[\cfrac{d}{dx} y(x)=f(x,y(x))\]
                \[\cfrac{dy}{dx}=g(x)H(y)\]
                \[\cfrac{dy}{dx}=\cfrac{g(x)}{h(y)}\]
                \[h(y)dy=g(x)dx\]
                \[\int h(y) dy=\int g(x) dx\]
                \begin{equation}
                    \begin{aligned}
                        \int h(y)\ dy&=\int h(y(x))\cfrac{dy}{dx} dx\\
                        &=\int h(y(x))\cfrac{g(x)}{h(y(x))} dx\\
                        &=\int g(x)\ dx
                    \end{aligned}
                \end{equation}
            \paragraph{Unlimited Population Growth}
            \paragraph{Radioactivity}
            \[\text{Half-life}=\cfrac{\ln 2}{k}\]
            \paragraph{Heat Transfer: Newton's Law of Cooling}
            \[\cfrac{dH}{dt}=-k(H-H_s)\]
            \[\cfrac{dy}{dt}=\cfrac{d}{dt}(H-H_s)=\cfrac{dH}{dt}-\cfrac{d}{dt}(H_s)\]
            \[=\cfrac{dH}{dt}\]
            \[=-k(H-H_s)\]
            \[=-ky\]
        \subsection{Indeterminate Forms and L'H$\bf{\hat{o}}$pital's Rule}
            \paragraph{Indeterminate Form $0/0$} Suppose that $f(a)=g(a)=0$, that $f$ and $g$ are differentiable on an open interval $I$ containing $a$, and that $g'(x)\ne 0$ on $I$ if $x\ne a$. Then
            \[\lim\limits_{x\to a}\cfrac{f(x)}{g(x)}=\lim\limits_{x\to a}\cfrac{f'(x)}{g'(x)}\]
            assuming that the limit on the right side of this equation exists.
            \paragraph{Indeterminate Forms $\infty/\infty$, $\infty/0$, $\infty\cdot 0$, $\infty-\infty$}
            $x\to a$ may be replaced by the one-sided limits $x\to a^{+}$ or $x\to a^{-}$
            \paragraph{Indeterminated Power}
            \text{}\\
            \par If $\lim\limits_{x\to a}$ in $f(x)=L$ then 
            \[\lim\limits_{x\to a}f(x)=\lim\limits_{x\to a}e^{\ln f(x)} =e^L\]
            \par Here $a$ may be either finite or infinite.
            \paragraph{Example} Apply l'hopital's Rule to show that $\lim\limits_{x\to 0^{+}} (1+x)^{1/x}=e$
            \subparagraph{Solution}
            \begin{equation}
                \begin{aligned}
                    \ln f(x)&=\ln (1+x)^{1/x}=\cfrac{1}{x}\ln (1+x)\\
                    \lim\limits_{x\to 0^+} \ln f(x)&=\lim\limits_{x\to 0^+}\cfrac{\ln (1+x)}{x}\\
                    &=\lim\limits_{x\to 0^+}\cfrac{\cfrac{1}{1+x}}{1}\\
                    &=1\\
                \end{aligned}                
            \end{equation}
            \paragraph{Proof of L'H$\bf{\hat{o}}$pital's Rule}
            \begin{equation}
                \begin{aligned}
                    \lim\limits_{x\to a}\cfrac{f(x)}{g(x)}&=\lim\limits_{x\to a}\cfrac{f'(a)(x-a)+\epsilon_1 (x-a)}{g'(a)(x-a)+\epsilon_2(x-a)}\\
                    &=\lim\limits_{x\to a} \cfrac{f'(a)+\epsilon_1}{g'(a)+\epsilon_2}\\
                    &=\cfrac{f'(a)}{g'(a)}\\
                    &=\lim\limits_{x\to a} \cfrac{f'(x)}{g'(x)}
                \end{aligned}                
            \end{equation}
                \subparagraph{Cauchy's Mean Value Theorem} Suppose functions $f$ and $g$ are continuous on [a,b] and differentiable throughout $(a,b)$ and also suppose $g'(x)\ne 0$ throughout $(a,b)$. Then there exists a number $c$ in $(a,b)$ at which
                \[\cfrac{f'(c)}{g'(c)}=\cfrac{f(b)-f(a)}{g(b)-g(a)}\]
                \subparagraph{Proof}
                \[g'(c)=\cfrac{g(b)-g(a)}{b-a}=0\]
                \[F(x)=f(x)-f(a)-\cfrac{f(b)-f(a)}{g(b)-g(a)}[g(x)-g(a)]\]
                \[F'(c)=f'(c)-\cfrac{f(b)-f(a)}{g(b)-g(a)}[g'(c)]=0\]
                \[\cfrac{f'(c)}{g'(c)}=\cfrac{f(b)-f(a)}{g(b)-g(a)}\]
                \subparagraph{Proof of L'H$\bf{\hat{o}}$pital's Rule}
                \[\cfrac{f'(c)}{g'(c)}=\cfrac{f(x)-f(a)}{g(x)-g(a)}\]
                \[\cfrac{f'(c)}{g'(c)}=\cfrac{f(x)}{g(x)}\]
                \[\lim\limits_{x\to a^+}\cfrac{f(x)}{g(x)}=\lim\limits_{c\to a^+}\cfrac{f'(c)}{g'(c)}=\lim\limits_{x\to a^+}\cfrac{f'(x)}{g'(x)}\]
        \subsection{Inverse Trigonometric Functions}
            \paragraph{Defining the Inverses}
            \paragraph{The Arcsine and Arccosine Functions}
            \paragraph{Identities Involving Arcsine and Arccosine}
            \[\cos^{-1}x+\cos^{-1}(-x)=\pi\]
            \[\sin^{=1}x+\cos^{-1}x=\pi/2\]
            \paragraph{Inverses of $\tan x$, $\cot x$, $\sec x$, and $\csc x$}
            \[\sec^{-1}x=\cos ^{-1}\bigg(\cfrac{1}{x}\bigg)=\cfrac{\pi}{2}-\sin^{-1}\bigg(\cfrac{1}{x}\bigg)\]
            \paragraph{The Derivative of $y=\sin ^{-1} u$}
            \begin{equation}
                \begin{aligned}
                    (f^{-1})'(x)&=\cfrac{1}{f'(f^{-1}(x))}\\
                    &=\cfrac{1}{\cos (\sin ^{-1}x)}\\
                    &=\cfrac{1}{\sqrt{1-\sin ^2(\sin ^{-1}x)}}\\
                    &=\cfrac{1}{\sqrt{1-x^2}}
                \end{aligned}
            \end{equation}
            \[\cfrac{d}{dx}(sin^{-1}u)=\cfrac{1}{\sqrt{1-u^2}}\cfrac{du}{dx},\quad |u|<1\]
            \paragraph{The Derivative of $y=\tan ^{-1} u$}
            \begin{equation}
                \begin{aligned}
                    (f^{-1})'(x)&=\cfrac{1}{f'(f^{-1}(x))}\\
                    &=\cfrac{1}{\sec^2(\tan^{-1}x)}\\
                    &=\cfrac{1}{1+\tan^2(\tan^{-1}x)}\\
                    &=\cfrac{1}{\sqrt{1+x^2}}
                \end{aligned}
            \end{equation}
            \[\cfrac{d}{dx}(sin^{-1}u)=\cfrac{1}{\sqrt{1+u^2}}\cfrac{du}{dx},\quad |u|<1\]
            \paragraph{The Derivative of $y=\sec^{-1}u$}
            \begin{equation}
                \begin{aligned}
                    y&=\sec ^{-1}x\\
                    \sec y&=x\\
                    \cfrac{d}{dx}(\sec y)&=\cfrac{d}{dx}x\\
                    \sec y\tan y\cfrac{dy}{dx}&=1\\
                    \cfrac{dy}{dx}&=\cfrac{1}{\sec y\tan y}\\
                    \cfrac{dy}{dx}&=\pm \cfrac{1}{x\sqrt{x^2-1}}\\
                \end{aligned}
            \end{equation}
            \[\cfrac{d}{dx}\sec ^{-1} x=\left\{
                \begin{aligned}
                    &+\cfrac{1}{x\sqrt{x^2-1}}\quad\text{if}x>1\\
                    &-\cfrac{1}{x\sqrt{x^2-1}}\quad\text{if}x<-1\\
                \end{aligned}
            \right.\]
            \[\cfrac{d}{dx}\sec ^{-1} x=\cfrac{1}{|x|\sqrt{x^2-1}}\]
            \[\cfrac{d}{dx}\sec ^{-1} u=\cfrac{1}{|u|\sqrt{u^2-1}}\cfrac{du}{dx}\]
            \paragraph{Inverse Function-Inverse Confunction Identities}
            \[\cos ^{-1}x=\pi/2-\sin^{-1}x\]
            \[\cot ^{-1}x=\pi/2-\tan^{-1}x\]
            \[\csc ^{-1}x=\pi/2-\sec^{-1}x\]
            \paragraph{Integration Formulas}
            \begin{equation}
                \begin{aligned}
                    &\int\cfrac{du}{\sqrt{a^2-u^2}}=\sin ^{-1}\bigg(\cfrac{u}{a}\bigg)+C\\
                    &\int\cfrac{du}{a^2+u^2}=\cfrac{1}{a}\tan^{-1}\bigg(\cfrac{u}{a}\bigg)+C\\
                    &\int\cfrac{du}{u\sqrt{u^2-a^2}}=\cfrac{1}{a}\sec^{-1}\bigg|\cfrac{u}{a}\bigg|+C\\
                \end{aligned}
            \end{equation}
        \subsection{Hyperbolic Functions}
            \paragraph{Definitions and Identities}
                \[\sinh x=\cfrac{e^x-e^{-x}}{2}\]
                \[\cosh x=\cfrac{e^x+e^{-x}}{2}\]
                \begin{equation}
                    \begin{aligned}
                        2\sinh x\cosh x&=2\bigg(\cfrac{e^x-e^{-x}}{2}\bigg)\bigg(\cfrac{e^x+e^{-x}}{2}\bigg)\\
                        &=\cfrac{e^{2x}-e^{-2x}}{2}\\
                        &=\sinh 2x
                    \end{aligned}                    
                \end{equation}
                \[\tanh x=\cfrac{e^x+e^{-x}}{e^x-e^{-x}}\]
                \[\coth x=\cfrac{e^x-e^{-x}}{e^x+e^{-x}}\]
                \[\text{sech} x=\cfrac{2}{e^x+e^{-x}}\]
                \[\text{csch} x=\cfrac{2}{e^x-e^{-x}}\]
                \[\cosh ^2 x-\sinh^2 x=1\]
                \[\cosh 2x=\cosh^2x+\sinh^2x\]
                \[\sinh 2x=2\sinh x\cosh x\]
                \[\cosh ^2x=\cfrac{\cosh 2x+1}{2}\]
                \[\sinh ^2x=\cfrac{\cosh 2x-1}{2}\]
                \[\tanh^2x=1-\text{sech}^2 x\]
                \[\coth^2x=1+\text{csch}^2 x\]
            \paragraph{Derivatives and Integrals of Hyperbolic Functions}
                \[\cfrac{d}{dx}(\sinh u)=\cosh u\cfrac{du}{dx}\]
                \[\cfrac{d}{dx}(\cosh u)=\sinh u\cfrac{du}{dx}\]
                \[\cfrac{d}{dx}(\tanh u)=\text{sech}^2 u\cfrac{du}{dx}\]
                \[\cfrac{d}{dx}(\coth u)=-\text{csch}^2 u\cfrac{du}{dx}\]
                \[\cfrac{d}{dx}(\text{csch } u)=-\text{sech }u \tanh u\cfrac{du}{dx}\]
                \[\cfrac{d}{dx}(\text{csch }u)=-\text{csch }u \coth u\cfrac{du}{dx}\]
                \text{}\\
                \[\int\sinh u\ du=\cosh u+C\]
                \[\int\cosh u\ du=\sinh u+C\]
                \[\int\text{sech}^2 u\ du=\tanh u+C\]
                \[\int\text{csch}^2 u\ du=-\coth u+C\]
                \[\int\text{sech } u\tanh u\ du=-\text{sech }u+C\]
                \[\int\text{csch } u\coth u\ du=-\text{csch }u+C\]
            \paragraph{Inverse Hyperbolic Function}
                \[\text{sech}^{-1}=\cosh^{-1}\cfrac{1}{x}\]
                \[\text{csch}^{-1}=\sinh^{-1}\cfrac{1}{x}\]
                \[\coth^{-1}=\tanh^{-1}\cfrac{1}{x}\]
            \paragraph{Derivatives of Inverse Hyperbolic Functions}
                \[\cfrac{d(\sinh ^{-1}u)}{dx}=\cfrac{1}{\sqrt{1+u^2}}\cfrac{du}{dx}\quad\]
                \[\cfrac{d(\cosh ^{-1}u)}{dx}=\cfrac{1}{\sqrt{1-u^2}}\cfrac{du}{dx}\quad u>1\]
                \[\cfrac{d(\tanh ^{-1}u)}{dx}=\cfrac{1}{1-u^2}\cfrac{du}{dx}\quad |u|<1\]
                \[\cfrac{d(\coth ^{-1}u)}{dx}=\cfrac{1}{1-u^2}\cfrac{du}{dx}\quad |u|>1\]
                \[\cfrac{d(\text{sech} ^{-1}u)}{dx}=\cfrac{1}{u\sqrt{1-u^2}}\cfrac{du}{dx}\quad 0<u<1\]
                \[\cfrac{d(\text{csch} ^{-1}u)}{dx}=\cfrac{1}{|u|\sqrt{1+u^2}}\cfrac{du}{dx}\quad u\ne 0\]
                \[\int \cfrac{du}{\sqrt{a^2+u^2}}=\sinh ^{-1}(\cfrac{u}{a})+C,\qquad a>0\]
                \[\int \cfrac{du}{\sqrt{u^2-a^2}}=\cosh ^{-1}(\cfrac{u}{a})+C,\qquad u>a>0\]
                \[\int \cfrac{du}{a^2-u^2}=\left\{
                    \begin{aligned}
                        \cfrac{1}{a}\tanh ^{-1}(\cfrac{u}{a})+C,\quad u^2<a^2\\
                        \cfrac{1}{a}\coth ^{-1}(\cfrac{u}{a})+C,\quad u^2>a^2\\
                    \end{aligned}
                \right.\]
                \[\int \cfrac{du}{u \sqrt{a^2-u^2}}=-\cfrac{1}{a}\text{sech}^{-1}(\cfrac{u}{a})+C,\quad 0<u<a\]
                \[\int \cfrac{du}{u\sqrt{a^2+u^2}}=-\cfrac{1}{a}\text{csch}^{-1}|\cfrac{u}{a}|+C,\qquad 0<u<a\]
        \subsection{Relative Rates of Growth}
            \paragraph{Growth Rate of Functions} Let $f(x)$ and $g(x)$ be positive for $x$ sufficiently large\\
            1. $f$ grows faster than $g$ as $x\to \infty$ if
            \[\lim\limits_{x\to \infty}\cfrac{f(x)}{g(x)}=\infty\]
            2. $f$ grows faster than $g$ as $x\to \infty$ if
            \[\lim\limits_{x\to \infty}\cfrac{f(x)}{g(x)}=0\]
            3. $f$ and $g$ grow at the same rate as $x\to \infty$ if
            \[\lim\limits_{x\to \infty}\cfrac{f(x)}{g(x)}=L\]
            where $L$ is finite and positive.
            \paragraph{Order and Oh-Notation}
                \subparagraph{Definition}
            A function $f$ is \textbf{of smaller order than} $\bf{g}$ as $x\to \infty$ if $\lim\limits_{x\to \infty}\cfrac{f(x)}{g(x)}=0$. We indicate this by writing $f=o(g)$.
                \subparagraph{Definition} Let $f(x)$ and $g(x)$ be positive for $x$ sufficiently large. Then $f$ is of at most the order of $g$ as $x\to \infty$ if there is a positive integer $M$ for which
                \[\cfrac{f(x)}{g(x)}\le M\]
                for $x$ sufficiently large. We indicate this by writing $f=O(g)$.
            \paragraph{Sequential vs. Binary Search}

    \newpage
    \section{Techniques of Integration}
        \subsection{Using Basic Integration Formulas}
            \paragraph{Basic Integration formulas}
            \[\int k\ dx=kx +C\]
            \[\int x^n\ dx=\cfrac{x^{n+1}}{n+1} +C\]
            \[\int \cfrac{dx}{x}=\ln |x| +C\]
            \[\int e^x\ dx=e^x +C\]
            \[\int a^x\ dx=\cfrac{a^x}{\ln a} +C\]
            \[\int \sin x\ dx=-\cos x +C\]
            \[\int \cos x\ dx=\sin x +C\]
            \[\int \sec^2 x\ dx=\tan x +C\]
            \[\int \csc^2 x\ dx=-\cot x +C\]
            \[\int \sec x\tan x\ dx=\sec x +C\]
            \[\int \csc x\cot x\ dx=-\csc x +C\]
            \[\int \tan x\ dx=\ln |\sec x| +C\]
            \[\int \cot x\ dx=\ln |\sin x| +C\]
            \[\int \sec x\ dx=\ln |\sec x+\tan x| +C\]
            \[\int \csc x\ dx=-\ln |\csc x+\cot x| +C\]
            \[\int \sinh x\ dx=\cosh x +C\]
            \[\int \cosh x\ dx=\sinh x +C\]
            \[\int \cfrac{dx}{\sqrt{a^2-x^2}}=\sin^{-1}(\cfrac{x}{a}) +C\]
            \[\int \cfrac{dx}{a^2+x^2}=\cfrac{1}{a}\tan^{-1}(\cfrac{x}{a}) +C\]
            \[\int \cfrac{dx}{x\sqrt{x^2-a^2}}dx=\cfrac{1}{a}\sec^{-1}|\cfrac{x}{a}| +C\]
            \[\int \cfrac{dx}{\sqrt{a^2+x^2}}dx=\sinh^{-1}(\cfrac{x}{a}) +C\]
            \[\int \cfrac{dx}{\sqrt{x^2-a^2}}dx=\cosh^{-1}(\cfrac{x}{a}) +C\]
        \subsection{Integration by Parts}
            \paragraph{Product Rule in Integral Form}
            \[\int \cfrac{d}{dx}[f(x)g(x)]dx=\int[f'(x)g(x)+f(x)g'(x)]dx\]
            \[\int \cfrac{d}{dx}[f(x)g(x)]dx=\int[f'(x)g(x)]+\int[f(x)g'(x)]dx\]
            \[\int f(x)g'(x)dx=\int \cfrac{d}{dx}[f(x)g(x)]dx+\int f'(x)g(x)\]
            \[\int f(x)g'(x)dx=f(x)g(x)-\int f'(x)g(x)dx\]
            \subparagraph{Integration by Parts Formula}
            \[\int u\ dv=uv-\int v\ du\]
                \subparagraph{Example} Find
                \[\int x\cos x\ dx\]
                \subparagraph{Solution}
                    \[u=x,\quad dv=\cos x\ dx\]
                    \[du=dx,\quad v=\sin x\]
                \par Then
                    \[\int x\cos x\ dx=x\sin x-\int \sin x\ dx=x\sin x+\cos x+C\]
            \paragraph{Evaluating Definite Integrals by Parts}
                \subparagraph{Integration by Parts Formula for Definite Integrals}
                \[\int_a^bf(x)g'(x)dx=f(x)g(x)\Bigg]_a^b=\int_a^bf'(x)g(x)dx\]
            \paragraph{Tabular Integration Can Simplify Repeated Integrations}
        \subsection{Trigonometric Integrals}
            \paragraph{Products of Power of Sines and Cosinea}
            \[\int \sin^mx\cos^nx\ dx\]
            \par \textbf{Case 1: }If \textbf{m is odd}
            \[\sin^mx=\sin^{2k+1}x=(\sin^2x)^k \sin x=(1-\cos^2)^k\sin x\]
            \par Then we combine the single $\sin x$ with $dx$ in the integral and set $\sin x\ dx$ equal to $-d(cosx)$.\\
            \par \textbf{Case 2: }If \textbf{m is even and n is odd}
            \[\cos^nx=\cos^{2k+1}x=(\cos^2x)^k\cos x=(1-\sin^2x)^k\cos x\]
            \par We then combine the single $\cos x$ with $dx$ and set $\cos x\ dx$ equal to $d(\sin x).$
            \\
            \par \textbf{Case 3: }If \textbf{both m and n are even}
            \[\sin^2x=\cfrac{1-\cos 2x}{2},\qquad\cos^2x=\cfrac{1+\cos 2x}{2} \]
            \par to reduce the integrand to one in lower power of $\cos 2x.$
            \paragraph{Eliminating Square Roots}
            \paragraph{Integrals of Powers of $\tan x$ and $\sec x$}
                \subparagraph{Example} Evaluate 
                \[\int \tan^4 x\ dx\]
                \subparagraph{Solution}
                \begin{equation}
                    \begin{aligned}
                        \int \tan^4x\ dx&=\int \tan^2x\cdot\tan ^2 x\ dx\\
                        &=\int \tan^2x\cdot(\sec ^2x-1) dx\\
                        &=\int \tan^2x\sec^2x\ dx-\int\tan^2x\ dx\\
                        &=\int\tan^2\sec^2\ dx-\int(\sec^2-1)\ dx\\
                        &=\int\tan^2\sec^2\ dx-\int\sec^2x\ dx+\int dx\\
                        u=\tan x,\quad &du=\sec^2x\ dx\\
                        \int u^2du&=\cfrac{1}{3}u^3+C_1\\ 
                        \int \tan^4x\ dx&=\cfrac{1}{3}\tan^3x-\tan x+x+C\\
                    \end{aligned}
                \end{equation}
            \paragraph{Products of Sines and Cosines}
            \[\sin mx\sin nx=\cfrac{1}{2}[\cos(m-n)x-\cos(m+n)x]\]
            \[\sin mx\cos nx=\cfrac{1}{2}[\sin(m-n)x+\sin(m+n)x]\]
            \[\cos mx\cos nx=\cfrac{1}{2}[\cos(m-n)x+\cos(m+n)x]\]
        \subsection{Trigonometric Substitutions}
            \paragraph{Procedure for a Trigonometric Substitution}
            \text{}\\
            \par 1. Write down the substitution for $x$, calculate the differential $dx$, and specify the selected values of $\theta$ for the substitution.
            \par 2. Substitute the trigonometric expression and the calculated differential into the integrand, and then simplify the results algebraically.
            \par 3. Integrate the trigonometric integral, keeping in mind the restrictions on the angle $\theta$ for reversibility.
            \par 4. Draw an appropriate reference triangle to reverse the substitution in the integration result and convert it back to the original variable $x$.
        \subsection{Integration of Rational Functions by Partial Fractions}
            \paragraph{the method of partial fractions}
            \[\cfrac{5x-3}{x^2-2x-3}=\cfrac{A}{x+1}+\cfrac{B}{x-3}\]
            \[A=2,\qquad B=3\]
            \paragraph{General Description of the Method}
                \subparagraph{Method of Partial Fractions when $f(x)/g(x)$ is Proper}
                \text{}\\
                \par 1. Let $x-r$ be a linear factor of $g(x)$. Suppose that $(x-r)^m$ is the highest power of $x-r$ that divides $g(x)$. Then, to this factor, assign the sum of the $m$ partial fractions:
                \[\cfrac{A_1}{(x-r)}+\cfrac{A_2}{(x-r)^2}+\cdots+\cfrac{A_m}{(x-r)^m}\]
                \par Do this for each distinct linear factor of $g(x)$.
                \par 2. Let $x^2+px+q$ be an irreducible quadratic factor of $g(x)$ so that $x^2+px+q$ has no real roots. Suppose that $(x^2+px+q)^n$ is the highest power of this factor that divides $g(x)$. Then, to this factor, assign the sum of the $n$ partial fractions:
                \[\cfrac{B_1x+C_1}{(x^2+px+q)}+\cfrac{B_2x+C_2}{(x^2+px+q)^2}+\cdots+\cfrac{B_nx+C_n}{(x^2+px+q)^n}\]
                \par Do this for each distinct quadratic factor of $g(x)$.
                \par 3. Set the original fraction $f(x)/g(x)$ equal to the sum of all these partial fractions. Clear the resulting equation of fractions and arrange the terms in decreasing power of $x$.
                \par 4. Equate the coefficients of corresponding powers of x and solve the resulting equations for the undetermined coefficients.
            \paragraph{The Heavisde "Cover-up" method for Linear Factors}
                When the degree of the polynomial $f(x)$ is less than the degree of $g(x)$ and
                \[g(x)=(x-r_1)(x-r_2)\cdots(x-r_n)\]
                is a product of $n$ distinct linear factors, each raised to the first power, there is a quick way to expand $f(x)/g(x)$ by partial fractions.
                \subparagraph{Heaviside Method}
                \par 1. \textit{Write the quotient with $g(x)$ factored:}
                \[\cfrac{f(x)}{g(x)}=\cfrac{f(x)}{(x-r_1)(x-r_2)\cdots(x-r_n)}\]
                \par 2. \textit{Cover the factors $(x-r_1)$ of $g(x)$ one at a time }, each time replacing all the uncovered $x$'s by the number $r_i$. This gives a number $A_i$ for each root $r_i$:
                \begin{equation}
                    \begin{aligned}
                        A_1&=\cfrac{f(r_1)}{(r_1-r_2)\cdots(r_1-r_n)}\\
                        A_2&=\cfrac{f(r_2)}{(r_2-r_1)(r_2-r_3)\cdots(r_2-r_n)}\\
                        \vdots\\
                        A_n&=\cfrac{f(r_n)}{(r_n-r_1)(r_n-r_2)\cdots(r_n-r_{n-1})}\\
                    \end{aligned}
                \end{equation}
                \par 3. \textit{Write the partial fraction expansion of $f(x)/g(x)$ as}
                \[\cfrac{f(x)}{g(x)}=\cfrac{A_1}{(x-r_1)}+\cfrac{A_2}{(x-r_2)}+\cdots+\cfrac{A_n}{(x-r_n)}\]
            \paragraph{Other Ways to Determine the Coefficients}
        \subsection{Integral Tables and Computer Algebra Systems}
            \paragraph{Integral Tables}
            \paragraph{Reduction Formulas}
            \paragraph{Integration with CAS}
            \paragraph{Nonelemtary Integrals}
            \[\text{erf}(x)=\cfrac{2}{\sqrt{\pi}}\int_0^x e^{-t^2}dt\]
            \[\int \sin x^2dx\]
            \[\int \sqrt{1+x^4}dx\]
        \subsection{Numerical integration}
            \paragraph{Trapezoidal Approximation}
            \[\Delta x=\cfrac{b-a}{n}\quad \textbf{step size } \text{or} \textbf{ mesh size}\]
            \[\Delta x\bigg(\cfrac{y_{i-1}+y_i}{2}\bigg)=\cfrac{\Delta x}{2}(y_{i-1}+y_i)\]
            \begin{equation}
                \begin{aligned}
                    T&=\cfrac{1}{2}(y_0+y_1)\Delta x+\cfrac{1}{2}(y_1+y_2)\Delta x+\cdots+\cfrac{1}{2}(y_{n-1}+y_n)\Delta x\\
                    &=\Delta x\bigg(\cfrac{1}{2}y_0+y_1+y_2+\cdots +y_{n-1}+\cfrac{1}{2}y_n\bigg)\\
                    &=\cfrac{\Delta x}{2}(y_0+2y_1+2y_2+\cdots+2y_{n-1}+y_n)
                \end{aligned}
            \end{equation}
            where
            \[y_0=f(a),\quad y_1=f(x_1), \cdots,\quad y_{n-1}=f(x_{n-1}),\quad y_n=f(b)\]
            The Trapezoidal Rule says: Use $T$ to estimate teh integral of $f$ from $a$ to $b$.
            \paragraph{Simpson's Rule: Approximations Using Parabolas}\text{}\\
                To approximate $\int_a^bf(x)dx$, use
                \[S=\cfrac{\Delta x}{3}(y_0+4y_1+2y_2+4y_3+\cdots+2y_{n-2}+4y_{n-1}+y_n)\]
                The $y$'s are the values of $f$ at the partition points
                \[x_i=a+i\Delta x\]
                The number $n$ is even, and $\Delta x=(b-a)/n$.
            \paragraph{Error Analysis}
                \subparagraph{Error Estimates in the Trapezoidal and Simpson's Rules}
                If $f''$ is continuous and $M$ is any upper bound for the values of $|f''|$ on $[a,b]$
                \[|E_T|\le\cfrac{M(b-a)^3}{12n^2}\quad \textit{Trapezoidal Rule}\]
                If $f^{(4)}$ is continuous and $M$ is any upper bound for the values of $|f^{(4)}|$ on $[a,b]$ 
                \[|E_S|\le\cfrac{M(b-a)^5}{180n^4}\quad \textit{Simpson's Rule}\]
        \subsection{Improper Integrals}
            \paragraph{Infinite Limits of Integration}
            \[\int_a^\infty f(x)dx=\lim\limits_{b\to \infty}\int_a^b f(x)dx\]
            \[\int_{-\infty}^bf(x)dx=\lim\limits_{b\to \infty}\int_a^b f(x)dx\]
            \[\int_{-\infty}^\infty f(x)dx=\int_{-\infty}^cf(x)dx+\int_c^\infty f(x) dx\]
            \paragraph{The Integral $\int_1^\infty \cfrac{dx}{x^p}$}
                \subparagraph{Solution} If $p\ne 1$
                \[\int_1^b\cfrac{dx}{x^p}=\cfrac{x^{-p+1}}{-p+1}\bigg]_1^b=\cfrac{1}{1-p}(b^{-p+1}-1)=\cfrac{1}{1-p}(\cfrac{1}{b^{p-1}}-1)\]
                Thus,
                \begin{equation}
                    \begin{aligned}
                        \int_1^\infty\cfrac{dx}{x^p}&=\lim\limits_{b\to\infty}\int_1^b\cfrac{dx}{x^p}\\
                        &=\lim\limits_{b\to \infty}\bigg[\cfrac{1}{1-p}\bigg(\cfrac{1}{b^{p-1}-1}\bigg)\bigg]\\
                        &=\left\{
                            \begin{aligned}
                                &\cfrac{1}{p-1},\quad p>1\\
                                &\infty,\qquad\  p<1\\
                            \end{aligned}
                        \right.
                    \end{aligned}
                \end{equation}
                because
                \[\lim\limits_{b\to\infty}\cfrac{1}{b^{p-1}}=\left\{
                    \begin{aligned}
                        &0,\quad\ \, p>1\\
                        &\infty,\quad p<1\\
                    \end{aligned}    
                \right.
                \]
                \par If $p=1$, the integral also diverges:
                \begin{equation}
                    \begin{aligned}
                        \int_1^\infty \cfrac{dx}{x^p}&\\
                        &=\int_1^\infty\cfrac{dx}{x}\\
                        &=\lim\limits_{b\to\infty}\int_1^b\cfrac{dx}{x}\\
                        &=\lim\limits_{b\to\infty}\ln x\bigg]_1^b\\
                        &=\lim\limits_{b\to\infty}(\ln b-\ln 1)=\infty
                    \end{aligned}
                \end{equation}
            \paragraph{Integrands with Vertical Asymptotes}
                \subparagraph{Imporoper Integrals of Type II}\text{}\\
                \par 1. If $f(x)$ is continuous on $(a,b]$ and discontinuous at $a$, then
                \[\int_a^bf(x)dx=\lim\limits_{c\to a^+}\int_c^bf(x)dx\]
                \par 2. If $f(x)$ is continuous on $[a,b)$ and discontinuous at $b$, then
                \[\int_a^bf(x)dx=\lim\limits_{c\to b^-}\int_a^cf(x)dx\]
                \par 3. If $f(x)$ is discontinuous at $c$, where $a<c<b$, and continuous on $[a,c)\cup(c,b]$, then
                \[\int_a^bf(x)dx=\int_a^cf(x)dx+\int_c^bf(x)dx\]
            \paragraph{Improper Integrals with a CAS}
            \paragraph{Tests for convergence and Divergence}
                \subparagraph{Direct Comparison Test} Let $f$ and $g$ be continuous on $[a,\infty)$ with $0\le f(x)\le g(x)$ for all $x\ge a$. Then
                \par 1. $\int_a^\infty(x)dx$ converges if $\int_a^\infty(x)dx$ converges.
                \par 2. $\int_a^\infty(x)dx$ diverges if $\int_a^\infty(x)dx$ diverges.
                \subparagraph{Limit Comparison Test} If the positive functions $f$ and $g$ are continuous on $[a,\infty)$, and if 
                \[\lim\limits_{x\to \infty}\cfrac{f(x)}{g(x)}=L,\quad 0<L<\infty\]
                then
                \[\int_a^\infty f(x)dx\quad\text{and}\quad\int_a^\infty g(x)dx\]
                both converge or both diverge.
        \subsection{Probability}
            \paragraph{Random Variables}
            A \textbf{random variable} is a function $X$ that assigns a numerical value to each outcome in a sample place.
            \par Random variables that have only finitely many values are called \textbf{discrete} random variables. A \textbf{continuous random variable} can take on values in an entire interval, and it is associated with a \textit{distribution function}.
            \paragraph{Probability Distributions}
                A \textbf{probability density function} for a continuous random variable is a function $f$ defined over $(-\infty,\infty)$ and having the following properties:
                \par 1. $f$ is continuous, except possibly at a finite number of points.
                \par 2. $f$ is nonnegative, so $f\ge 0$.
                \par 3. $\int_{-\infty}^\infty f(x)dx=1$.
                If $X$ is a continuous random variable with probability density function $f$, the \textbf{probability} that $X$ assumes a value in the interval between $X=c$ and $X=d$ is the area integral
                \[P(c\le X\le d)=\int_c^df(X)dX\]
            \paragraph{Exponentially Decreasing Distributions}
            \[f(x)=\left\{
                \begin{aligned}
                    &0\quad\qquad x<0\\
                    &ce^{-cx}\ \ \ \, x\ge 0
                \end{aligned}
            \right.\]
            \paragraph{Expected Values, Means, and Medians}
            The \textbf{expected value} or \textbf{mean} of a continuous random variable $X$ with probability density function $f$ is the number
            \[\mu=E(X)=\int_{-\infty}^\infty Xf(X)dx\]
                \subparagraph{Exponential Density Function for a Random Variable $X$ with Mean $\mu$}
                \[f(X)=\left\{
                    \begin{aligned}
                        &a\qquad\qquad\quad X<0\\
                        &\mu^{-1}e^{-X/\mu}\,\quad X\ge 0
                    \end{aligned}
                \right.\]
                \subparagraph{Definition} The \textbf{median} of a continuous random variable $X$ with probability density function $f$ is the number $m$ for which
                \[\int_{-\infty}^m f(X)dX=\cfrac{1}{2}\qquad\text{and}\qquad\int_m^\infty f(X)dX=\cfrac{1}{2}\]
            \paragraph{Variance and Standard Deviation} The \textbf{variance} of a random variable $X$ with probability density function $f$ is the expected value of $(X-\mu)^2f(X)dX$
            \[\text{Var}(X)=\int_{-\infty}^\infty(X-\mu)^2f(X)dX\]
            The \textbf{standard deviation} of $X$ is
            \[\sigma_X=\sqrt{\text{Var}(X)}=\sqrt{\int_{-\infty}^\infty(X-\mu)^2f(X)dX}\]
            \paragraph{Uniform Distributions}
            \[f(x)=\cfrac{1}{b-a},\quad a\le x\le b\]
            \paragraph{Normal Distributions}
            \[f(x)=\cfrac{1}{\sigma\sqrt{2\pi}}\cdot e^{-(x-\mu)^2/2\sigma^2}\]
    
    \newpage
    \section{First-Order Differential Equations}
        \subsection{Solutions, Slope Fields, and Euler's Method}
        A \textbf{first-order differential equation} is an equation
        \[\cfrac{dy}{dx}=f(x,y)\]
        \[\cfrac{d}{dx}y(x)=f(x,y(x))\]
            \paragraph{Slope Fields: Viewling Solution Curves}
            \paragraph{Euler's Method}
        \subsection{First-Order Linear Equations}
            \[\cfrac{dy}{dx}+P(x)y=Q(x)\quad \text{Standard form}\]
            \paragraph{Solving Linear Equations}
            \begin{equation}
                \begin{aligned}
                    \cfrac{dy}{dx}+P(x)y&=Q(x)\\
                    v(x)\cfrac{dy}{dx}+P(x)v(x)y&=v(x)Q(x)\\
                    \cfrac{d}{dx}(v(x)\cdot y)&=v(x)Q(x)\\
                    v(x)\cdot y&=\int v(x)Q(x)dx\\
                    y&=\cfrac{1}{v(x)}\int v(x)Q(x)dx\\
                    &\\
                    \cfrac{d}{dx}(vy)&=v\cfrac{dy}{dx}+Pvy\\
                    v\cfrac{dy}{dx}+y\cfrac{dv}{dx}&=v\cfrac{dy}{dx}+Pvy\\
                    y\cfrac{dy}{dx}&=Pvy\\
                    \cfrac{dv}{dx}&=Pv\\
                    \cfrac{dv}{v}&=P\ dx\\
                    \int \cfrac{dv}{v}&=\int P\ dx\\
                    \ln v&=\int P\ dx\\
                    e^{\ln v}&=e^{\int P\ dx}\\
                    v&=e^{\int P\ dx}\\
                \end{aligned}
            \end{equation}
            \par To solve the linear equation $y'+P(x)y=Q(x)$, multiply both sides by the integrating factor $v(x)=e^{\int P(x)dx}$ and integrate both sides.
                \subparagraph{Example} Solve the equation
                \[x\cfrac{dy}{dx}=x^2+cy,\quad x>0\]
                \subparagraph{Solution}
                \[\cfrac{dy}{dx}-\cfrac{3}{x}y=x\]
                \begin{equation}
                    \begin{aligned}
                        v(x)=e^{\int P(x)dx}&=e^{\int(-3/x) dx}\\
                        &=e^{-3\ln |x|}\\
                        &=e^{-3\ln x}\\
                        &=e^{\ln x^{-3}}=\cfrac{1}{x^3}\\
                        \cfrac{1}{x^3}\cdot(\cfrac{dy}{dx}-\cfrac{3}{x}y)&=\cfrac{1}{x^3}\cdot x\\
                        \cfrac{1}{x^3}\cfrac{dy}{dx}-\cfrac{3}{x^4}y&=\cfrac{1}{x^2}\\
                        \cfrac{d}{dx}(\cfrac{1}{x^3}y)&=\cfrac{1}{x^2}\\
                        \cfrac{1}{x^3}y&=\int \cfrac{1}{x^2} dx\\
                        \cfrac{1}{x^3}y&=-\cfrac{1}{x}+C\\
                        y&=-x^2+Cx^3,\quad x>0\\
                    \end{aligned}
                \end{equation}
            \paragraph{$\bf{RL}$ Circuits}
                \[L\cfrac{di}{dt}+Ri=V\]
        \subsection{Applications}
            \paragraph{Motion with Resistance Proportional to Velocity}
            \[F=m\cfrac{dv}{dt}\]
            \[m\cfrac{dv}{dt}=-kv\]
            \[v=v_0e^{-(k/m)t}\]
            \[\cfrac{ds}{dt}=v_0e^{-(k/m)t},\quad s(0)=0\]
            \[s=-\cfrac{v_0m}{k}e^{-(k/m)t}+C\]
            \par Substituting $s=0$ when $t=0$ gives
            \[0=-\cfrac{v_0m}{k}+C\quad\text{and}\quad C=\cfrac{v_0m}{k}\]
            \[s(t)=-\cfrac{v_0m}{k}e^{-(k/m)t}+\cfrac{v_0m}{k}=\cfrac{v_0m}{k}(1-e^{-(k/m)t})\]
            \[\lim\limits_{t\to \infty}s(t)=\lim\limits_{t\to \infty}\cfrac{v_0m}{k}(1-e^{-(k/m)t})\]
            \[=\cfrac{v_0m}{k}(1-0)=\cfrac{v_0m}{k}\]
            \[\text{Distance coasted}=\cfrac{v_0m}{k}\]
            \paragraph{Inaccuracy of the Exponential Population Growth Model}
            \paragraph{Orthogonal Trajectories}
            An \textbf{orthogonal trajectory} of a family of curves is a curve that intersects each curve of the family at right angles, or textit{orthogonally}.
        \subsection{Graphical Solutions of Autonomous Equations}
            \paragraph{Equilibrium Values and Phase Lines}
            If $dy/dx=g(y)$ is an autonomous differential equation, then the values of $y$ for which $dy/dx=0$ are called \textbf{equilibrium values} or \textbf{rest points}.
                \subparagraph{Example} Draw a phase line for the equation
                \[\cfrac{dy}{dx}=(y+1)(y-2)\]
                \subparagraph{Solution}
                \par 1. \textit{Draw a number line for $y$ and mark the equilibrium values $y=-1$ and $y=2$, where $dy/dx=0$.}
                \par 2. \textit{Identify and label the intervals where $y'>0$ and $y'<0$}.
                \par 3. \textit{Calculate $y''$ and mark the intervals where $y''>0$ and $y''<0$.}
                \[y'=(y+1)(y-2)=y^2-y-2\]
                \begin{equation}
                    \begin{aligned}
                        y''&=\cfrac{d}{dx}(y')\\
                        &=\cfrac{d}{dx}(y^2-y-2)\\
                        &=2yy'-y'\\
                        &=(2y-1)y'\\
                        &=(2y-1)(y+1)(y-2)\\
                    \end{aligned}
                \end{equation}
                \par 4. \textit{Sketch an assortment of solution curves in the xy-plane.}
            \paragraph{Stable and Unstable Equilibria}
            \paragraph{Newton's Law of Cooling}
                \[\cfrac{dH}{dt}=-k(H-H_s),\quad k>0\]
                \[\cfrac{d^2H}{dt^2}=-k\cfrac{dH}{dt}\]
            \paragraph{A Falling Body Encountering Resistance}
            \paragraph{Logistic Population Growth}
                \[\cfrac{dP}{dt}=kP\]
                \[\cfrac{dP}{dt}=r(M-P)P=rMP-rP^2\]
        \subsection{Systems of Equations and Phase Planes}
            \paragraph{Phase Planes}
            \[\cfrac{dx}{dt}=F(x,y)\]
            \[\cfrac{dy}{dt}=G(x,y)\]
            \paragraph{A Competitive-Hunter Model}
            \[\cfrac{dx}{dt}=(a-by)x\]
            \[\cfrac{dy}{dt}=(m-nx)y\]
            \paragraph{Limitations of Phase-Plane Analysis Method}
            \paragraph{Another Type of Behavior}
            The system
            \[\cfrac{dx}{dt}=y+x-x(x^2+y^2)\]
            \[\cfrac{dy}{dt}=-x+y-y(x^2+y^2)\]
            \[\text{Limit cycle:}\quad x^2+y^2=1\]

    \newpage
    \section{Infinite Sequences and Series}
        \subsection{Sequence}
            \paragraph{Representing Sequences}
            \paragraph{Convergence and Divergence}
                \subparagraph{Definitions} The sequence $\{a_n\}$ \textbf{converges} to the number $L$ if for every positive number $\epsilon$ there corresponds an integer $N$ such that for all $n$.
                \[n>N\quad \quad |a_n-L|<\epsilon\]
                If no such number $L$ exists, we say that $\{a_n\}$ \textbf{diverges}.
                \par If ${a_n}$ converges to $L$, we write $\lim\limits_{n\to\infty}a_n=L$, or simply $a_n\to L$, and call $L$ the \textbf{limit} of the sequence.
                \subparagraph{Definitions} The sequence $\{a_n\}$ \textbf{diverges to infinity} if for every number $M$ there is an integer $N$ such that for all $n$ larger than $N$, $a_n>M$. If this condition holds we write
                \[\lim\limits_{n\to\infty}a_n=\infty\quad \text{or}\quad a_n\to \infty\]
                Similarly, if for every number $m$ there is an integer N such that for all $n>N$ we have $a_n<m$, then we say ${a_n}$ \textbf{diverges to negative infinity} and write
                \[\lim\limits_{n\to\infty}a_n=-\infty\quad\text{or}\quad a_n\to -\infty\]
            \paragraph{Calculating Limits of Sequences}
                \begin{equation}
                    \begin{aligned}
                        &\lim\limits_{n\to\infty}(a_n+b_n)=A+B\\
                        &\lim\limits_{n\to\infty}(a_n-b_n)=A-b\\
                        &\lim\limits_{n\to\infty}(k\cdot b_n)=k\cdot B\\
                        &\lim\limits_{n\to\infty}(a_n\cdot b_n)=A\cdot B\\
                        &\lim\limits_{n\to\infty}(\frac{a_n}{b_n})=\frac{A}{B}\\
                    \end{aligned}
                \end{equation}
            \paragraph{The Sandwich Theorem for Sequences} Let $\{a_n\},\{b_n\}, and \{c_n\}$ be sequences of real numbers. If $a_n\le b_n\le c_n$ holds for all $n$ beyond some index $N$, and if $\lim\limits_{n\to\infty}a_n=\lim\limits_{n\to\infty}c_n=L$, then $\lim\limits_{n\to\infty}b_n=L$ also.
            \paragraph{The Continuous Function Theorem for Sequences} Let $\{a_n\}$ be a sequence of real numbers. If $a_n\to L$ and if $f$ is a function that is continuous at $L$ and defined at all $a_n$, then $f(a_n)\to f(L)$.
            \paragraph{Using L'Hopital's Rule} Suppose that $f(x)$ is a function defined for all $x\ge n_0$ and that $\{a_n\}$ is a sequence of real numbers such that $a_n=f(n)$ for $n\ge n_0$. Then
            \[\lim\limits_{x\to\infty}f(x)=L\quad \Rightarrow \quad \lim\limits_{n\to\infty}a_n=L\]
            \paragraph{Commonly Occuring Limits}
            \[\lim\limits_{n\to\infty}\cfrac{\ln n}{n}=0\]
            \[\lim\limits_{n\to\infty}\sqrt[n]{n}=1\]
            \[\lim\limits_{n\to\infty}x^{1/n}=1\]
            \[\lim\limits_{n\to\infty}x^n=0\]
            \[\lim\limits_{n\to\infty}\bigg(1+\cfrac{x}{n}\bigg)^n=e^x\]
            \[\lim\limits_{n\to\infty}\cfrac{x^n}{n!}=9\]
            \paragraph{Bounded Monotonic Sequences}
                \subparagraph{The Monotonic Sequence Theorem} If a sequence $\{a_n\}$ is both bounded and monotonic, then the sequence converges.
        \subsection{Infinite Series}
            \[a_1+a_2+\cdots+a_n+\cdots=\sum_{n=1}^\infty a_n=L\]
            \paragraph{Geometric Series}
                \[a+ar+ar^2+\cdots+ar^{n-1}+\cdots=\sum_{n=1}^\infty ar^{n-1}\]
                \par If $|r|<1$
                \[\sum_{n=1}^\infty ar^{n-1}=\cfrac{a}{1-r},\qquad|r|<1\]
                \par If $|r|\ge 1$, the series diverges.
            \paragraph{The $n$th-Term Test for a Divergent Series}
                \subparagraph{Theorem} If $\sum_{n=1}^\infty a_n$ converges, then $a_n\to 0$.
                \subparagraph{The $n$th-Term Test for Divergence}
                $\sum_{n=1}^\infty a_n$ diverges if $\lim\limits_{n\to\infty}a_n$ fails to exist or is different from zero.
            \paragraph{Combining Series}
                \subparagraph{Theorem}
                \[\sum (a_n+b_n)=\sum a_n+\sum b_n=A+B\]
                \[\sum (a_n-b_n)=\sum a-n-\sum b_n=A-B\]
                \[\sum ka_n=k\sum a_n=kA\quad (\text{any number } k)\]
            \paragraph{Adding or Deleting Terms}
                \[\sum_{n=1}^\infty a_n=a_1+a_2+\cdots+a_{k-1}+\sum_{n=k}^\infty a_n\]
            \paragraph{Reindexing}
                \[\sum_{n=1}^\infty a_n=\sum_{n=1+h}^\infty a_{n-h}=a_1+a_2+\cdots\]
                \[\sum_{n=1}^\infty a_n=\sum_{n=1-h}^\infty a_{n+h}=a_1+a_2+\cdots\]
        \subsection{The Integral Test}
            \paragraph{Nondecreasing Partial Sum} A series $\sum_{n=1}^\infty a_n$ of nonnegative terms converges if and only if its partial sums are bounded from above.
            \paragraph{The Integral Test}
            Let $\{a_n\}$ be a sequence of positive terms. Suppose that $a_n=f(n)$, where $f$ is a continuous, positive,decreasing function of $x$ for all $x\ge N$($N$ a positive integer). Then the series $\sum_{n=N}^\infty =a_n$ and the integral $\int_N^\infty f(x)dx$ both converge or both diverge. 
                \subparagraph{$p$-series}
                    \[\sum_{n=1}^\infty \cfrac{1}{n^p}=\cfrac{1}{1^p}+\cfrac{1}{2^p}+\cfrac{1}{3^p}+\cdots+\cfrac{1}{n^p}+\cdots\]
                    \par converges if $p>1$, and diverges if $p\le 1$
                \subparagraph{Solution}
                If $p>1$, then $f(x)=1/x^p$ is a positive decreasing function of $x$. Since
                \begin{equation}
                    \begin{aligned}
                        \int_1^\infty\cfrac{1}{x^p}dx&=\int_1^\infty x^{-p}dx\\
                        &=\lim\limits_{b\to\infty}\bigg[\cfrac{x^{-p+1}}{-p+1}\bigg]_1^b\\
                        &=\cfrac{1}{1-p}\lim\limits_{b\to\infty}\bigg(\cfrac{1}{b^{p-1}}-1\bigg)\\
                        &=\cfrac{1}{1-p}(0-1)\\
                        &=\cfrac{1}{p-1}\\
                    \end{aligned}
                \end{equation}
                \par If $p\le 0$, the series diverges by the $n$th-term test.
                \par If $0<p<1$,
                \[\int_1^\infty \cfrac{1}{x^p}dx=\cfrac{1}{1-p}\lim\limits_{b\to\infty}(b^{1-p}-1)=\infty\]
                \par If $p=1$,
                \[1+\cfrac{1}{2}+\cfrac{1}{3}+\cdots+\cfrac{1}{n}+\cdots\]
                \par is the \textbf{Harmonic Series}.
            \paragraph{Error Estimation}
                \subparagraph{Bounds for the Remainder in the Integral Test}
                \text{}\\
                \par Suppose $\{a_k\}$ is a sequence of positive terms with $a_k=f(k)$, where $f$ is a continuous positive decreasing function of $x$ for all $x\ge n$, and that $\sum a_n$ converges to $S$. Then the remainder $R_n=S-s_n$ satisfies the inequalities.
                \[\int_{n+1}^\infty f(x)dx\le R_n\le \int_n^\infty f(x) dx\]
        \subsection{Comparison Tests}
            \paragraph{The Comparison Test} Let $\sum a_n$, $\sum c_n$, and $\sum d_n$ be series with nonnegative terms. Suppose that for some integer $N$
            \[d_n\le a_n\le c_n\quad \text{for all}\quad n>N\]
            \par (a) If $\sum c_n$ converges, then $\sum a_n$ also converges.
            \par (b) If $\sum d_n$ diverges, then $\sum a_n $ also diverges.
            \paragraph{The Limit Comparison Test}
            Suppose that $a_n>0$ and $b_n>0$ for all $n\ge N$($N$ an integer). 
            \par 1. If $\lim\limits_{n\to\infty}\cfrac{a_n}{b_n}=c>0$, then $\sum a_n$ and $\sum b_n$ both converge or both diverge.
            \par 2. If $\lim\limits_{n\to\infty}\cfrac{a_n}{b_n}=0$ and $\sum b_n$ converges, then $\sum a_n$ converges.
            \par 3. If $\lim\limits_{n\to\infty}\cfrac{a_n}{b_n}=\infty$ and $\sum b_n$ diverges, then $\sum a_n$ diverges.
        \subsection{Absolute Convergence; The Ratio and Root Tests}
            \paragraph{Definition} A series $\sum a_n$ \textbf{converges absolutely} (is \textbf{absolutely convergent}) if the corresponding series of absolute values, $\sum|a_n|$, converges.
            \paragraph{The Absolute Convergence Test} If $\sum_{n=1}^\infty |a_n|$ converges, then $\sum_{n=1}^\infty a_n$ converges.
            \paragraph{The Ratio Test} Let $\sum a_n$ be any series and suppose that
            \[\lim\limits_{n\to\infty}\bigg|\cfrac{a_{n+1}}{a_n}\bigg|=\rho\]
            Then
            \par (a) the series converges absolutely if $\rho <1$,
            \par (b) the series diverges if $\rho >1$ or $\rho$ is infinite,
            \par (c) the test is inconclusive if $\rho=1$.
            \paragraph{The Root Test}
            Let $\sum a_n$ be any series and suppose that 
            \[\lim\limits_{n\to\infty}\sqrt[n]{|a_n|}=\rho\]
            Then
            \par (a) the series converges absolutely if $\rho <1$,
            \par (b) the series diverges if $\rho >1$ or $\rho$ is infinite,
            \par (c) the test is inconclusive if $\rho =1$
        \subsection{Alternating Series and Conditional Convergence}
            \paragraph{The Alternating Series Test} The series
            \[\sum_{n=1}^\infty(-1)^{n+1}u_n=u_1-u_2+u_3-u_4+\cdots\]
            converges if all three of the following conditions are satisfied:
            \par 1. The $u_n$'s are all positive.
            \par 2. The positive $u_n$'s are (eventually) nonincreasing: $u_n\ge u_{n+1}$ for all $n\ge N$, for some integer $N$.
            \par 3. $u_n\to 0$
            \paragraph{The Alternating Series Estimation Theorem} If the alternating series $\sum_{n=1}^\infty (-1)^{n+1}u_n$ satisfies the three conditions of The Alternating Series Test, then for $n\ge N$,
            \[s_n=u_1-u_2+\cdots+(-1)^{n+1}u_n\]
            approximates the sum $L$ of the series with an error whose absolute value is less than $u_{n+1}$, the absolute value of the first unused term. Furthermore, the sum $L$ lies between any two successive partial sums $s_n$ and $s_{n+1}$, and the remainder, $L-s_n$, has the same sign as the first unused term.
            \paragraph{Conditional Convergence}
                \subparagraph{Definition} A convergent series that is not absolutely convergent is \textbf{conditionally convergent}.
            \paragraph{Rearranging Series}
                \subparagraph{The Rearrangement Theorem for Absolutely Convergent Series} If $\sum_{n=1}^\infty a_n$ converges absolutely, and $b_1$, $b_2$,$\cdots$,$b_n$,$\cdots$ is any arrangement of the sequence $\{a_n\}$, then $\sum b_n$ converges absolutely and
                \[\sum_{n=1}^\infty b_n=\sum_{n=1}^\infty a_n\]
        \subsection{Power Series}
            \paragraph{Power Series and Convergence}
                \subparagraph{Definitions} \textbf{A power series about $\bf{x=0}$} is a series of the form
                \[\sum_{n=0}^\infty c_n x^n=c_0+c_1x+c_2x^2+\cdots+c_nx^n+\cdots\]
                \par \textbf{A power series about $\bf{x=a}$} is a series of the form
                \[\sum_{n=0}^\infty c_n(x-a)^n=c_0+c_1(x-a)+c_2(x-a)^2+\cdots+c_n(x-a)^n+\cdots\]
                \par in which the \textbf{center} $a$ and the \textbf{coefficients} $c_0,c_1,c_2,\cdots,c_n,\cdots$ are constants.
            \paragraph{The convergence Theorem for Power Series} If the power series $\sum_{n=0}^\infty a_n x^n=a_0+a_1x+a_2x^2+\cdots$ converges at $x=c\ne 0$, then it converges absolutely for all $x$ with $|x|<|c|$. If the series diverges at $x=d$, then it diverges for all $x$ with $|x|>|d|$.
            \paragraph{The Radius of Convergence of a Power Series}
            The convergence of the series $\sum c_n(x-a)^n$ is described by one of the following three cases:
            \par 1. There is a positive number $R$ such that the series diverges for $x$ with $|x-a|>R$ but converges absolutely for $x$ with $|x-a|<R$. The series may or may not converge at either of the endpoints $x=a-R$ and $x=a+R$.
            \par 2. The series converges absolutely for every $x$ ($R=\infty$).
            \par 3. The series converges at $x=a$ and diverges elsewhere ($R=0$)\\
            \par  $R$ is called the \textbf{radius of convergence} of the power series, and the interval of radius $R$ centered at $x=a$ is called the \textbf{interval of convergence}.
            \paragraph{How to Test a Power Series for Convergence}
            \text{}\\
            \par 1. \textit{Use the Ratio Test(or Root Test) to find the interval where the series converges absolutely}. Ordinarily, this is an open interval
            \[|x-a|<R\quad \text{or}\quad a-R<x<a+R\]
            \par 2. \textit{If the interval of absolute convergence is finite, test for convergence or divergence at each endpoint}. Use a Comparison Test, the Integral Test, or the Alternating Series Test.
            \par 3. \textit{If the interval of absolute convergence is $a-R<x<a+R$, the series diverges for $|x-a|>R$}(it does not even converge conditionally) because the $n$th term does not approach zero for those values of $x$.
            \paragraph{Operations on Power Series}
                \subparagraph{The Series Multiplication Theorem for Power Series} If $A(x)=\sum_{n=0}^\infty a_nx^n$ and $B(x)=\sum_{n=0}^\infty b_nx^n$ converge absolutely for $|x|<R$, and 
                \[c_n=a_0b_n+a_1b_{n-1}+a_2b_{n-2}+\cdots+a_{n-1}b_1+a_nb_0=\sum_{k=0}^na_kb_{n-k}\]
                then $\sum_{n=0}^\infty c_n x^n$ converges absolutely to $A(x)B(x)$ for $|x|<R$:
                \[\Bigg(\sum_{n=0}^\infty a_n x^n\Bigg)\cdot\Bigg(\sum_{n=0}^\infty b_n x^n\Bigg)=\sum_{n=0}^\infty c_nx^n\]
                \subparagraph{Theorem} If $\sum_{n=0}^\infty a_nx^n$ converges absolutely for $|x|<R$, then $\sum_{n=0}^\infty a_n(f(x))^n$ converges absolutely for any continuous function $f$ on $|f(x)|<R$.
                \subparagraph{The Term-by-Term Differentiation Theorem} If $\sum c_n(x-a)^n$ has radius of convergence $R>0$, it defines a function
                \[f(x)=\sum_{n=0}^\infty c_n(x-a)^n\qquad \text{on the interval}\qquad a-R<x<a+R\]
                This function $f$ has derivatives of all orders inside the interval, and we obtain the derivatives by differentiating the original series term by term:
                \[f'(x)=\sum_{n=1}^\infty nc_n(x-a)^{n-1}\]
                \[f''(x)=\sum_{n=2}^\infty n(n-1)c_n(x-a)^{n-2}\]
                and so on. Each of these derived series converges at every point of the interval $a-R<x<a+R$.
                \subparagraph{The Term-by-Term Integration Theorem} Suppose that
                \[f(x)=\sum_{n=0}^\infty c_n(x-a)^n\]
                converges for $a-R<x<a+R(R>0)$. Then
                \[\sum_{n=0}^\infty c_n\cfrac{(x-a)^{n+1}}{n+1}\]
                converges for $a-R<x<a+R$ and
                \[\int f(x)dx=\sum_{n=0}^\infty c_n \cfrac{(x-a)^{n+1}}{n+1}+C\]
                for $a-R<x<a+R$.
        \subsection{Taylor and Maclaurin Series}
            \paragraph{Series Representations}
            \[f(x)=\sum_{n=0}^\infty a_n(x-a)^n=a_0+a_1(x-a)+a_2(x-a)^2+\cdots+a_n(x-a)^n+\cdots\]
            \[f'(x)=a_1+2a_x(x-a)+3a_3(x-a)^2+\cdots+na_n(x-a)^{n-1}+\cdots\]
            \[f''(x)=1\cdot 2a_x+2\cdot 3a_3(x-a)+3\cdot 4a_4(x-a)^2+\cdots\]
            \[f'''(x)=1\cdot 2\cdot 3 a_3+2\cdot 3\cdot 4 a_4(x-a)+3\cdot 4\cdot 5a_5(x-a)^2+\cdots\]
            \[f^{(n)}(x)=n!a_n+\text{a sum of terms with (x-a) as a factor.}\]
            Since these equations all hold at $x=a$, we have
            \[f'(a)=a_1,\quad f''(a)=1\cdot 2a_2,\quad f'''(a)=1\cdot 2\cdot 3 a_3\]
            \[f^{(n)}(a)=n!a_n\]
            \[a_n=\cfrac{f^{(n)}(a)}{n!}\]
            \[f(x)=f(a)+f'(a)(x-a)+\frac{f''(a)}{2!}(x-a)^2+\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n+\cdots\]
            \paragraph{Taylor and Maclaurin Series}
                \subparagraph{Definitions} Let $f$ be a function with derivatives of all orders throughout some interval containing $a$ as an interior point. Then the \textbf{Taylor series generated by $\bf{f}$ at $\bf{x=a}$} is
                \[\sum_{k=0}^\infty \cfrac{f^{(k)}(a)}{k!}(x-a)^k=f(a)+f'(a)(x-a)+\cfrac{f''(a)}{2!}(x-a)^2+\cdots+\cfrac{f^{(n)}(a)}{n!}(x-a)^n+\cdots\]
                The \textbf{Maclaurin series of $\bf{f}$} is the Taylor series generated by $f$ at $x=0$, or
                \[\sum_{k=0}^\infty \cfrac{f^{(k)}(0)}{k!}x^k=f(a)+f'(0)x+\cfrac{f''(0)}{2!}x^2+\cdots+\cfrac{f^{(n)}(0)}{n!}x^n+\cdots\]
            \paragraph{Taylor Polynomials}
            \[P_1(x)=f(a)+f'(a)(x-a)\]
                \subparagraph{Definition} Let $f$ be a function with derivatives of order $k$ for $k=1,2,\cdots,N$ in some interval containing $a$ as an interior point. Then for any integer $n$ from 0 through $N$, the \textbf{Taylor polynomial of order $\bf{n}$} generated by $f$ at $x=a$ is the polynomial
                \[P_n(x)=f(a)+f'(a)(x-a)+\cfrac{f''(a)}{2!}(x-a)^2+\cdots+\cfrac{f^{(k)}(a)}{k!}(x-a)^k+\cdots+\cfrac{f^{(n)}(a)}{n!}(x-a)^n\]
        \subsection{Convergence of Taylor Series}
            \paragraph{Taylor's Theorem} If $f$ and its first $n$ derivatives $f',f'',\cdots,f^{(n)}$ are continuous on the closed interval between $a$ and $b$, and $f^{(n)}$ is differentiable on the open interval between $a$ and $b$, then there exists a number $c$ between $a$ and $b$ such that
            \[f(b)=f(a)+f'(a)(b-a)+\cfrac{f''(a)}{2!}(b-a)^2+\cdots+\cfrac{f^{(n)}(a)}{n!}(b-a)^n+\cfrac{f^{(n+1)}(c)}{(n+1)!}(b-a)^{n+1}\]
            \paragraph{Taylor's Formula}
            If $f$ has derivatives of all orders in an interval $I$ containing $a$, then for each positive integer $n$ and for each $x$ in $I$,
            \[f(x)=f(a)+f'(a)(x-a)+\cfrac{f''(a)}{2!}(x-a)^2+\cdots+\cfrac{f^{(n)}(a)}{n!}(x-a)^n+R_n(x)\]
            where
            \[R_n(x)=\cfrac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}\quad \text{for some $c$ between $a$ and $x$.}\]\\
            \par If $R_n(x)\to 0$ as $n\to \infty$ for all $x\in I$, we say that the Taylor series generated by $f$ at $x=a$ \textbf{converges}$f$ on $I$, and we write
            \[f(x)=\sum_{k=0}^\infty\cfrac{f^{(k)}(a)}{k!}(x-a)^k\]
            \subparagraph{Examples}
            \[e^x=1+x+\cfrac{x^2}{2!}+\cdots+\cfrac{x^n}{n!}+R_n(x)\]
            where $R_n(x)=\cfrac{e^c}{(n+1)!} x^{n+1}\quad$ for some $c$ between 0 and $x$. 
            \[|R_n(x)|\le \cfrac{|x|^{n+1}}{(n+1)!}\quad \text{when }x\le 0, e^c<1\]
            \[|R_n(x)|<e^x\cfrac{x^{n+1}}{(n+1)!}\quad \text{when } x>0, e^c<e^x\]
            \[\lim\limits_{n\to \infty} \cfrac{x^{n+1}}{(n+1)!}=0\quad \text{for every $x$}\]
            par $\lim\limits_{n\to\infty} R_n(x)=0$, and the series converges to $e^x$ for every $x$. Thus,
            \[e^x=\sum_{k=0}^\infty\cfrac{x^k}{k!}=1+x+\cfrac{x^2}{2!}+\cdots+\cfrac{x^k}{k!}+\cdots\]
            \[e=1+1+\cfrac{1}{2!}+\cdots+\cfrac{1}{n!}+R_n(1)\]
            \[R_n(1)=e^c\cfrac{1}{(n+1)!}<\cfrac{3}{(n+1)!}\]
            \paragraph{Estimating the Remainder}
                \subparagraph{The Remainder Estimation Theorem} If there is a positive constant $M$ such that $|f^{(n+1)}(t)|\le M$ for all $t$ between $x$ and $a$, inclusive, then the remainder term $R_n(x)$ in Taylor's Theorem satisfies the inequality.
                \[|R_n(x)|\le M\cfrac{|x-a|^{n+1}}{(n+1)!}\]
                If this inequality holds for every $n$ and the other conditions of Taylor's Theorem are satisfied by $f$, then the series converges to $f(x)$.
            \paragraph{A Proof of Taylor's Theorem}
        \subsection{The Binomial Series and Applications of Taylor Series}
            \paragraph{The Binomial Series for Power and Roots}
                \subparagraph{The Binomial Series}
                \text{}\\
                For $-1<x<1$,
                \[(1+x)^m=1+\sum_{k=1}^\infty
                    \begin{pmatrix}
                        m\\
                        k\\
                    \end{pmatrix}
                x^k
                \]
                where we define
                \[\begin{pmatrix}
                    m\\1=m
                \end{pmatrix},\quad
                \begin{pmatrix}
                    m\\2
                \end{pmatrix}=\cfrac{m(m-1)}{2!}\]
                and
                \[\begin{pmatrix}
                    m\\k
                \end{pmatrix}=\cfrac{m(m-1)(m-2)\cdots(m-k+1)}{k!}\quad \text{for }k\ge 3\]
            \paragraph{Evaluating Nonelementary Integrals}
                Taylor series can be used to express nonelementary integrals in terms of series. Integrals like $\int \sin x^2 dx$ arise in the study of the diffraction of light.
                \subparagraph{Example}
                \[\sin x^2=x^2-\cfrac{x^6}{3!}+\cfrac{x^{10}}{5!}-\cfrac{x^{14}}{7!}+\cdots\]
                \[\int\sin x^2 dx=C+\cfrac{x^3}{3}-\cfrac{x^7}{7\cdot 3!}+\cfrac{x^{11}}{11\cdot 5!}-\cdots\]
            \paragraph{Arctangent}
                \[\cfrac{d}{dx}\tan^{-1}x=\cfrac{1}{1+x^2}=1-x^2+x^4-x^6+\cdots\]
                \[\tan^{-1}x=x-\cfrac{x^3}{3}+\cfrac{x^5}{5}-\cfrac{x^7}{7}\]
                \[\tan^{-1}x=\sum_{n=0}^\infty\cfrac{(-1)^nx^{2n+1}}{2n+1},\quad |x|\le 1\]
                \par when $x=1$
                \[\cfrac{\pi}{4}=1-\cfrac{1}{3}+\cfrac{1}{5}-\cfrac{1}{7}+\cdots+\cfrac{(-1)^n}{2n+1}+\cdots\]
            \paragraph{Evaluating Indeterminate Forms}
            \paragraph{Euler's Identity}
                \[e^{i\theta}=1+\cfrac{i\theta}{1!}+\cfrac{i^2\theta^2}{2!}+\cfrac{i^3\theta^3}{3!}+\cfrac{i^4\theta^4}{4!}+\cfrac{i^5\theta^4}{4!}+\cdots\]
                \[=(1-\cfrac{\theta^2}{2!}+\cfrac{\theta^4}{4!}-\cfrac{\theta^6}{6!}+\cdots)+i(\theta-\cfrac{\theta^3}{3!}+\cfrac{\theta^5}{5!}-\cdots)=\cos \theta +i\sin \theta\]
                \subparagraph{Definition} For any real number $\theta$, $e^{i\theta}=\cos \theta+i\sin\theta$
                \[e^{i\pi}=-1\]
            \paragraph{Frequently used Taylor series}
            \[\cfrac{1}{1-x}=\sum_{n=0}^\infty x^n,\quad |x|<1\]
            \[\cfrac{1}{1+x}=\sum_{n=0}^\infty (-1)^nx^n,\quad |x|<1\]
            \[e^x=\sum_{n=0}^\infty \cfrac{x^n}{n!},\quad|x|<\infty\]
            \[\sin x=\sum_{n=0}^\infty\cfrac{(-1)^n x^{2n+1}}{(2n+1)!},\quad |x|<\infty\]
            \[\cos x=\sum_{n=0}^\infty\cfrac{(-1)^nx^{2n}}{(2n)!},\quad |x|<\infty\]
            \[\ln(1+x)=\sum_{n=1}^\infty\cfrac{(-1)^{n-1}x^n}{n},\quad -1<x\le 1\]
            \[\tan ^{-1}x=\sum_{n=0}^\infty\cfrac{(-1)^n x^{2n+1}}{2n+1},\quad |x|\le 1\]
    
    \newpage
    \section{Parametric Equations and Polar Coordinates}
        \subsection{Parametrizations of Plane Curves}
            \paragraph{Parametric Equations}
                \subparagraph{Definition} If $x$ and $y$ are given as functions
                \[x=f(t),\quad y=g(t)\]
                over an interval $I$ of $t$-values, then the set of points $(x,y)=(f(t),g(t))$ defined by these equations is a \textbf{parametric curve}. The equations are \textbf{parametric equations} for the curve.
                \par $t$: parameter
                \par $I$: parameter interval
            \paragraph{Cycloids}
                \[x=a(t-\sin t),\quad y=a(1-\cos t)\]
            \paragraph{Brachistochrones and Tautochrones}
                \[mgy=\cfrac{1}{2}mv^2-\cfrac{1}{2}m(0)^2\]
                \[v=\sqrt{2gy}\]
                \[\cfrac{ds}{dT}=\sqrt{2gy}\]
                \[dT=\cfrac{ds}{\sqrt{2gy}}=\cfrac{\sqrt{1+(dy/dx)^2}dx}{\sqrt{2gy}}\]
                \[T_f=\int_{x=0}^{x=a\pi}\sqrt{\cfrac{1+(dy/dx)^2}{2gy}}dx\]
        \subsection{Calculus with Parametric Curves}
            \paragraph{Tangents and Areas}
                \[\cfrac{dy}{dt}=\cfrac{dy}{dx}\cdot\cfrac{dx}{dt}\]
                \[\cfrac{dy}{dx}=\cfrac{dy/dt}{dx/dt}\]
                \[\cfrac{d^2y}{dx^2}=\cfrac{dy'/dt}{dx/dt}\]
            \paragraph{Length of a Parametrically Defined Curve}
                \[L=\int_a^b\sqrt{[f'(t)]^2+[g'(t)]^2}dt\]
                \[L=\int_a^b\sqrt{\bigg(\cfrac{dx}{dt}\bigg)^2+\bigg(\cfrac{dy}{dt}\bigg)^2}dt\]
                \subparagraph{Example} Find the perimeter of the ellipse $\cfrac{x^2}{a^2}+\cfrac{y^2}{b^2}=1$
                \begin{equation}
                    \begin{aligned}
                        (\cfrac{dx}{dt})^2+(\cfrac{dy}{dt})^2&=a^2\cos^2 t+b^2\sin^2t\\
                        &=a^2-(a^2-b^2)\sin^2t\\
                        &=a^2[1-e^2\sin^2t]\\
                        P&=4a\int_0^{\pi/2}\sqrt{1-e^2\sin^2t}dt\\
                        \sqrt{1-e^2\sin^2t}&=1-\cfrac{1}{2}e^2\sin^2t-\cfrac{1}{2\cdot 4}e^4\sin^4t-\cdots\\
                        P&=4a\int_0^{\pi/2}\sqrt{1-e^2\sin^2t}dt\\
                        &=4a[\cfrac{\pi}{2}-(\cfrac{1}{2}e^2)(\cfrac{1}{2}\cdot\cfrac{\pi}{2})-(\cfrac{1}{2\cdot 4}e^4)(\cfrac{1\cdot 3}{2\cdot 4}\cdot\cfrac{\pi}{2})-\cdots]\\
                        &=2\pi a[1-(\cfrac{1}{2})^2e^2-(\cfrac{1\cdot 3}{2\cdot 4})^2\cfrac{e^4}{3}-(\cfrac{1\cdot 3\cdot 5}{2\cdot 4\cdot 6})^2\cfrac{e^6}{5}-\cdots]
                    \end{aligned}
                \end{equation}
            \paragraph{Length of a Curve $y=f(x)$}
                \[(\cfrac{dx}{dt})^2+(\cfrac{dy}{dt})^2=1+[f'(t)]^2\]
            \paragraph{The Arc Length Differential}
                \[s(t)=\int_a^t\sqrt{[f'(z)]^2+[g'(z)]^2}dz\]
                \[\cfrac{ds}{dt}=\sqrt{[f'(t)]^2+[g'(t)]^2}=\sqrt{(\cfrac{dx}{dt})^2+(\cfrac{dy}{dt})^2}\]
                \[ds=\sqrt{dx^2+dy^2}\]
            \paragraph{Areas of Surfaces of Revolution}
                \subparagraph{Area of Surface of Revolution for Parametrized Curves}
                \text{}\\
                \par \textbf{1. Revolution about the x-axis}
                \[S=\int_a^b 2\pi y\sqrt{(\cfrac{dx}{dt})^2+(\cfrac{dy}{dt})^2}dt\]
                \par \textbf{2. Revolution about the y-axis}
                \[S=\int_a^b 2\pi x\sqrt{(\cfrac{dx}{dt})^2+(\cfrac{dy}{dt})^2}dt\]
        \subsection{Polar Coordinates}
            \paragraph{Definition of Polar Coordinates}
                \[P(r,\theta)\]
            \paragraph{Polar Equations and Graphs}
                \[x=r\cos \theta,\quad y=r\sin\theta,\quad r^2=x^2+y^2,\quad \tan\theta=\cfrac{y}{x}\]
        \subsection{Graphing Polar Coordinate Equations}
            \paragraph{Symmetry}
                \subparagraph{Symmetry Test for Polar Graphs in the Cartesian $xy$-Plane}
                \text{}\\
                \par 1. Symmetry about the x-axis
                \par 2. Symmetry about the y-axis
                \par 3. Symmetry about the the origin 
            \paragraph{Slope}
                \subparagraph{Slope of the Curve $r=f(\theta)$ in the Cartesian $xy$-Plane}
                \[\cfrac{dy}{dx}\bigg|_{(r,\theta)}=\cfrac{f'(\theta)\sin \theta+f(\theta)\cos \theta}{f'(\theta)\sin \theta-f(\theta)\cos \theta}\]
                provided $dx/d\theta\ne 0$ at $(r,\theta)$.
            \paragraph{Converting a Graph from the $r\theta-$ to $xy-$Plane}
                \par 1. first graph the function $r=f(\theta)$ in the Cartesian $r\theta$-plane
                \par 2. then use that Cartesian graph as a "table" and guide to sketch the polar coordinate graph in the $xy$-plane.
                \[x=r\cos\theta=f(\theta)\cos\theta\]
                \[y=r\sin\theta=f(\theta)\sin\theta\]
        \subsection{Areas and Lengths in Polar Coordinates}
            \paragraph{Area in the Plane}
                \[A_k=\cfrac{1}{2}r_k ^2\Delta \theta_k=\cfrac{1}{2}(f(\theta_k))^2\Delta\theta_k\]
                \[\sum_{k=1}^{n}A_k=\sum_{k=1}^n\cfrac{1}{2}(f(\theta_k))^2\Delta\theta_k\]
                \[A=\lim\limits_{||p||\to 0}\sum_{k=1}^n\cfrac{1}{2}(f(\theta_k))^2\Delta\theta_k\]
                \[=\int_\alpha^\beta \cfrac{1}{2}(f(\theta))^2d\theta\]
                \[dA=\cfrac{1}{2}r^2d\theta=\cfrac{1}{2}(f(\theta))^2d\theta\]
                \subparagraph{Area of the Region $0\le r_1(\theta)\le r\le r_2(\theta),\alpha\le\theta\le\beta$}
                \[A=\int_\alpha^\beta \cfrac{1}{2}r_2^2d\theta-\int_\alpha^\beta \cfrac{1}{2}r_1^2d\theta=\int_\alpha^\beta \cfrac{1}{2}(r_2^2-r_1^2)d\theta\]
            \paragraph{Length of a Polar Curve}
                \[L=\int_\alpha^\beta\sqrt{(\cfrac{dx}{d\theta})^2+(\cfrac{dy}{d\theta})^2}d\theta\]
                \[L=\int_\alpha^\beta\sqrt{r^2+(\cfrac{dr}{d\theta})^2}d\theta\]
        \subsection{Conic Sections}
            \paragraph{Parabolas}
                \subparagraph{Definition} A set that consists of all the points in a plane equidistant from a give fixed point and a given fixed line in the plane is a \textbf{parabola}. The fixed point is the \textbf{focus} of the parabola. The fixed line is the \textbf{directrix}.
            \[y=\cfrac{x^2}{4p}\quad\text{or}\quad x^2=4py\]
            \[y=-\cfrac{x^2}{4p}\quad\text{or}\quad x^2=-4py\]
            \paragraph{Ellipses}
                \subparagraph{Definition} An \textbf{ellipse} is the set of points in a plane whose distance from two fixed points in the plane have a constant sum. The two fixed points are the \textbf{foci} of the ellipse.
                \par The line through the foci of an ellipse is the ellipse's \textbf{focal axis}. The point on the axis halfway between the foci is the \textbf{center}. The points where the focal axis and ellipse cross are the ellipse's \textbf{vertices}.
                \[\cfrac{x^2}{a^2}+\cfrac{y^2}{b^2}=1\]
                \[\cfrac{dy}{dx}=-\cfrac{b^2x}{a^2y}\]
                \[c=\sqrt{a^2-b^2}\]
            \paragraph{Hyperbolas}
                \subparagraph{Definitions} A \textbf{hyperbola} is the set of points in a plane whose distances from two fixed points in the plane have a constant difference. The two fixed points are the \textbf{foci} of the hyperbola.
                \par The line through the foci of a hyperbola is the \textbf{focal axis}. The point on the axis halfway between the foci is the hyperbola's \textbf{center}. The points where hte focal axis and hyperbola cross are the \textbf{vertices.}
                \[\cfrac{x^2}{a^2}-\cfrac{y^2}{a^2}=1\]
                \[c^2=a^2+b^2\]
                \[\cfrac{dy}{dx}=\cfrac{b^2x}{a^2y}\]
                \[\textbf{asymptotes:}\quad y=\pm\cfrac{b}{a}x\]
        \subsection{Conics in Polar Coordinates}
            \paragraph{Eccentricity}
                \[(x^2/a^2)+(y^2/b^2)=1\quad(a>b)\]
                \[e=\frac{c}{a}=\cfrac{\sqrt{a^2-b^2}}{a}\]
                \newline
                \[(x^2/a^2)-(y^2/b^2)=1\quad(a>b)\]
                \[e=\frac{c}{a}=\cfrac{\sqrt{a^2+b^2}}{a}\]
                \par The \textbf{eccentricity} of a parabola is $e=1$.
                \[\text{Eccentricity}=\cfrac{\text{distance between foci}}{\text{distance between vertices}}\]
                \[PF=e\cdot PD\]
                \par The path traced by $P$ is
                \par (a) a parabola if $e=1$.
                \par (b) an ellipse of eccentricity $e$ if $e<1$.
                \par (c) a hyperbola of eccentricity $e$ if $e>1$.
            \paragraph{Polar Equations}
                \[PF=r\]
                \[r=e(k-r\cos\theta)\]
                \subparagraph{Polar Equation for a Conic with Eccentricity $e$}
                    \[r=\cfrac{ke}{1+e\cos\theta}\]
                    where $x=k>0$ is the vertical directrix. 
                \subparagraph{Polar Equation for the Ellipse with Eccentricity $e$ and Semimajor Axis $a$}
                    \[r=\cfrac{a(1-e^2)}{1+e\cos\theta}\]
                \subparagraph{The Standard Polar Equation for Lines}
                    \[r\cos(\theta-\theta_0)=r_0\]
                \subparagraph{Circle}
                    \[a^2=r_0^2+r^2-2r_0r\cos(\theta-\theta_0)\]
                    \[P_0(r_0,\theta_0)\]
        
    \newpage
    \section{Vectors and the Geometry of Space}
        \subsection{Three-Dimensional Coordinate System}
            \paragraph{Distance and Spheres in Space}
                \subparagraph{The Distance Between $P_1(x_1,y_1,z_1)$ and $P_2(x_2,y_2,z_2)$}
                \[|P_1P_2|=\sqrt{(x_2-x_1)^2+(y_2-y_1)^2+(z_2-z_1)^2}\]
                \subparagraph{The Standard Equation for the Sphere of Radius $a$ and Center $(x_0,y_0,z_0$}
                \[(x-x_0)^2+(y-y_0)^2+(z-z_0)^2=a^2\]
        \subsection{Vectors}
            \paragraph{Component Form}
                \[\overrightarrow{AB}\]
                \[\textbf{Two-dimensional vector:}\quad v=\langle v_1,v_2\rangle\]
                \[\textbf{Three-dimensional vector:}\quad v=\langle v_1,v_2,v_3\rangle\]
                \[\textbf{magnitude\\length:}\quad|v|=\sqrt{v_1^2+v_2^2+v_3^2}\]
                \[\textbf{zero vector 0}=\langle 0,0\rangle\quad\text{or}\quad =\langle 0,0,0\rangle\]
            \paragraph{Vector Algebro Operations}
                \[u=\langle u_1,u_2,u_3\rangle\quad v=\langle v_1,v_2,v_3\rangle\]
                \[u+v=\langle u_1+v_1,u_2+v_2,u_3+v_3\rangle\]
                \[ku=\langle ku_1,ku_2,ku_3\rangle\]
            \paragraph{Unit Vector}
                \[i=\langle 1,0,0\rangle\quad j=\langle 0,1,0\rangle\quad k=\langle 0,0,1\rangle\]
                \[v=\langle v_1,v_2,v_3\rangle=v_1i+v_2j+v_3k\]
                \[\textbf{Unit vector }\quad u=\cfrac{\overrightarrow{v}}{|\overrightarrow{v}|}\]
            \paragraph{Midpoint of a Line Segment}
                \[(\cfrac{x_1+x_2}{2}+\cfrac{y_1+y_2}{2}+\cfrac{z_1+z_2}{2})\]
            \paragraph{Applications}
        
        \subsection{The Dot Product}
            \paragraph{Angle Between Vectors}
                \[\theta=\cos^{-1}(\cfrac{u_1v_1+u_2v_2+u_3v_3}{|u||v|})\]
                \[u\cdot v=u_1v_1+u_2v_2+u_3v_3\]
                \[\theta=\cos^{-1}(\cfrac{u\cdot v}{|u||v|})\]
            \paragraph{Orthogonal Vector}
                Vector \textbf{u} and \textbf{v} are orthogonal if $u\cdot v=0$.
            \paragraph{Dot Product Properties and Vector Projections}
                \[u\cdot v=v\cdot u\]
                \[(cu)\cdot v=u\cdot(cv=c(u\cdot v))\]
                \[u\cdot(v+w)=u\cdot v+u\cdot w\]
                \[u\cdot u=|u|^2\]
                \[0\cdot u=0\]
                \[\text{proj}_v u=(\cfrac{u\cdot v}{|v|^2})v\]
                \[|u|\cos\theta =\cfrac{u\cdot v}{|v|}=u\cdot\cfrac{v}{|v|}\]
            \paragraph{Work}
                \[W=F\cdot D\]
        \subsection{The Cross Product}
            \paragraph{The Cross Product of Two Vectors in Space}
                \[u\times v=(|u||v|\sin\theta)n\]
                \subparagraph{Parallel Vectors}
                    Nonzero vectors \textbf{u} and \textbf{v} are parallel if and only if $u\times v=0$
                \subparagraph{Properties of the Cross Product}
                    \[(ru)\times(sv)=(rs)(u\times v)\]
                    \[u\times(v+w)=u\times v+u\times w\]
                    \[v\times u=-(u\times v)\]
                    \[(v+w)\times u=v\times u+w\times u\]
                    \[0\times u=0\]
                    \[u\times (v\times w)=(u\cdot w)v-(u\cdot v)w\]
            \paragraph{$|u\times v|$ is the Area of a Parallelogram}
                \[|u\times v|=|u||v||\sin\theta||n|=|u||v|\sin\theta\]
            \paragraph{Dertiminant Formula for $u\times v$}
                \[u=\langle u_1,u_2,u_3\rangle\qquad v=\langle v_1,v_2,v_3\rangle\]
                \[u\times v=(u_2v_3-u_3v_2)i-(u_1v_3-u_3v_1)j+(u_1v_2-u_2v_1)k=
                \begin{vmatrix}
                    i&j&k\\
                    u_1&u_2&u_3\\
                    v_1&v_2&v_3\\
                \end{vmatrix}\]
            \paragraph{Torque}
                \[\text{Torque vector}=(|r||F|\sin \theta)n\]
            \paragraph{Triple Scalar or Box Product}
                The product $(u\times v)\cdot w$ is called the \textbf{triple scalar product} of $u,v,w$
                \[|(u\times v)\cdot w|=|u\times v||w||\cos\theta\]
                \subparagraph{Calculating the Triple Scalar Product as a Determinant}
                    \[(u\times v)\cdot w=
                    \begin{vmatrix}
                        u_1&u_2&u_3\\
                        v_1&v_2&v_3\\
                        w_1&w_2&w_3\\
                    \end{vmatrix}\]
        \subsection{Lines and Planes in Space}
            \paragraph{Lines and Line Segments in Space}
                \subparagraph{Vector Equation for a Line}
                \[\textbf{r}(t)=\textbf{r}_0+t\textbf{v},\quad -\infty<t<\infty\]
                where \textbf{r} is the position vector of a point $P(x,y,z)$ on $L$ and $r_0$ is the position vector of $P_0(x_0,y_0,z_0)$.
                \subparagraph{Parametric Equations for a Line}
                The standard parametrization of the line through $P_0(x_0,y_0,z_0)$ parallel to $v=v_1 i+v_2j+v_3k$
                    \[x=x_0+tv_1,\qquad y=y_0+tv_2,\qquad z=z_0+tv_3,\qquad -\infty<t<\infty\]
            \paragraph{The Distance from a Point to a Line in Space}
                \[d=\cfrac{|\overrightarrow{PS}\times v|}{|v|}\]
            \paragraph{An Equation for a Plane in Space}
                The plane through $P_0(x_0,y_0,z_0)$ normal to $n=Ai+Bj+Ck$ has
                \[n\cdot \overrightarrow{P_0P}=0\]
                \[A(x-x_0)+B(y-y_0)+C(z-z_0)=0\]
                \[Ax+By+Cz=D,\quad D=Ax_0+By_0+Cz_0\]
            \paragraph{Lines of Intesection}
                Two planes are \textbf{parallel} if and only if their normals are parallel, or $n_1=kn_2$ for some scalar $k$.
            \paragraph{The Distance from a Point to a Plane}
                \[d=\bigg|\overrightarrow{PS}\cdot\cfrac{n}{|n|}\bigg|\]
            \paragraph{Angles Between Planes}
                \[\theta =\cos^{-1}(\cfrac{n_1\cdot n_2}{|n_1||n_2|})\]
        \subsection{Cylinders and Quadric Surfaces}
            \paragraph{Cylinders}
            \paragraph{Quadric Surface}
                \[Ax^2+By^2+Cz^2+Dz=E\]
                \subparagraph{Ellipsoid}
                    \[\cfrac{x^2}{a^2}+\cfrac{y^2}{b^2}+\cfrac{z^2}{c^2}=1\]
                \subparagraph{Elliptical Paraboloid}
                    \[\cfrac{x^2}{a^2}+\cfrac{y^2}{b^2}=\cfrac{z}{c}\]
                \subparagraph{Elliptical Cone}
                    \[\cfrac{x^2}{a^2}+\cfrac{y^2}{b^2}=\cfrac{z^2}{c^2}\]
                \subparagraph{Hyperboloid of One Sheet}
                    \[\cfrac{x^2}{a^2}+\cfrac{y^2}{b^2}-\cfrac{z^2}{c^2}=1\]
                \subparagraph{Hyperboloid of Two Sheets}
                    \[\cfrac{z^2}{c^2}-\cfrac{x^2}{a^2}-\cfrac{y^2}{b^2}=1\]
                \subparagraph{Hyperbolic Paraboloid}
                    \[\cfrac{y^2}{b^2}-\cfrac{x^2}{a^2}=\cfrac{z}{c},\quad c>0\]
    
    \newpage
    \section{Vector-Valued Functions and Motion in Space}
        \subsection{Curves in Space and Their Tangents}
            \[r(t)=f(t)i+g(t)j+h(t)k\]
            \paragraph{Limits and Continuity}
                \subparagraph{Definition}
                    Let $r(t)=f(t)i+g(t)j+h(t)k$ be a vector function with domain $D$, and $L$ a vector. We say that $r$ has \textbf{limit L} as $t$ approaches $t_0$ and write
                    \[\lim\limits_{t\to t_0}r(t)=L\]
                    if, for every number $\epsilon>0$, there exists a corresponding number $\delta>0$ such that for all $t\in D$
                    \[|r(t)-L|<\epsilon\quad \text{whenever}\quad 0<|t-t_0|<\delta\]
                    \[\lim\limits_{t\to t_0}r(t)=(\lim\limits_{t\to t_0}f(t))i+(\lim\limits_{t\to t_0}g(t))j+(\lim\limits_{t\to t_0}h(t))k\]
                \subparagraph{Definition} A vector function $r(t)$ is continuous at a point $t=t_0$ in its domain if $\lim\limits_{t\to t_0} r(t)=r(t_0)$. The function is continuous if it is continuous over its interval domain.
            \paragraph{Derivatives and Motion}
                \[r'(t)=\cfrac{dr}{dt}=\lim\limits_{\Delta t\to 0}\cfrac{r(t+\Delta t)-r(t)}{\Delta t}=\cfrac{df}{dt}i+\cfrac{dg}{dt}j+\cfrac{dh}{dt}k\]
            \paragraph{Differentiation Rules}
                \text{}\\
                \[\cfrac{d}{dt}C=0\]
                \[\cfrac{d}{dt}[cu(t)]=cu'(t)\]
                \[\cfrac{d}{dt}[f(t)u(t)]=f'(t)u(t)+f(t)u'(t)\]
                \[\cfrac{d}{dt}[u(t)+v(t)]=u'(t)+v'(t)\]
                \[\cfrac{d}{dt}[u(t)-v(t)]=u'(t)-v'(t)\]
                \[\cfrac{d}{dt}[u(t)\cdot v(t)]=u'(t)\cdot v(t)+u(t)\cdot v'(t)\]
                \[\cfrac{d}{dt}[u(t)\times v(t)]=u'(t)\times v(t)+u(t)\times v'(t)\]
                \[\cfrac{d}{dt}[u(f(t))]=f'(t)u'(f(t))\]
            \paragraph{Vector Functions of Constant Length}
                If $r$ is a differentiable vector function of a $t$ of a constant length, then
                \[r\cdot\cfrac{dr}{dt}=0\]
        \subsection{Integrals of Vector Function; Projectile Motion}
            \paragraph{Integrals of Vector Functions}
                \[\int r(t)dt=R(t)+C\]
                \[\int_a^br(t)dt=(\int_a^bf(t)dt)i+(\int_a^bg(t)dt)j+(\int_a^bh(t)dt)k\]
                \[\int_a^br(t)=R(t)\Bigg]_a^b=R(b)-R(a)\]
            \paragraph{The Vector and Parametric Equations for Ideal Projectile Motion}
                \[r=(v_0\cos\alpha)ti+((v_0\sin\alpha)t-\cfrac{1}{2}gt^2)j\]
                \[y_{\text{max}}=\cfrac{(v_0\sin\alpha)^2}{2g}\]
                \[t=\cfrac{2v_0\sin\alpha}{g}\]
                \[R=\cfrac{v_0^2}{g}\sin 2\alpha\]
                \[r=(x_0+(v_0\cos\alpha)t)i+(y_0+(v_0\sin\alpha)t-\cfrac{1}{2}gt^2)j\]
            \paragraph{Projectile Motion with Wind Gusts}
        \subsection{Arc Length in Space}
            \paragraph{Arc Length Along a Space Curve}
                \[L=\int_a^b\sqrt{(\cfrac{dx}{dt})^2+(\cfrac{dy}{dt})^2+(\cfrac{dz}{dt})^2}dt\]
                \subparagraph{Arc Length Formula}
                    \[L=\int_a^b|v|dt\]
                \subparagraph{Arc Length Parameter with Base Point $P(t_0)$}
                    \[s(t)=\int_{t_0}^t\sqrt{[x'(\tau)]^2+[y'(\tau)]^2+[z'(\tau)]^2}=\int_{t_0}^t|v(\tau)|d\tau\]
            \paragraph{Speed on a Smooth Curve}
                \[\cfrac{ds}{dt}=|v(t)|\]
            \paragraph{Unit Tangent Vector}    
                \[T=\cfrac{v}{|v|}\]
        \subsection{Curvature and Normal Vectors of a Curve}
            \paragraph{Curvature of a Plane Curve}
                \[\kappa=\cfrac{1}{|v|}\bigg|\cfrac{dT}{ds}\bigg|\]
                \[N=\cfrac{1}{\kappa}\cfrac{dT}{ds}=\cfrac{dT/dt}{|dT/dt|s}\]
                where $T=v/|v|$ is the unit tangent vector.
            \paragraph{Circle of a Curvature for Plane Curves}    
                \[\text{Radius of Curvature}=\rho=\cfrac{1}{\kappa}\]
            \paragraph{Curvature and Normal Vectors for Space Curves}
                \[\kappa=\bigg|\cfrac{dT}{ds}\bigg|=\cfrac{1}{|v|}\bigg|\cfrac{dT}{dt}\bigg|\]
                \[N=\cfrac{1}{\kappa}\cfrac{dT}{ds}=\cfrac{dT/dt}{|dT/dt|}\]
            \paragraph{Tangential and Normal Components of Acceleration}
                \paragraph{The TNB Frame}
                    \[B=T\times N\]
                \paragraph{Tangential and Normal Components of Accleration}
                    If the acceleration vector is written as 
                    \[a=a_TT+a_NN\]
                    then
                    \[a_T=\cfrac{d^2s}{dt^2}=\cfrac{d}{dt}|v|\qquad \text{and}\qquad a_N=\kappa(\cfrac{ds}{dt})^2=\kappa|v|^2\]
                    \subparagraph{Formula for Calculating the Normal Component of Acceleration}
                        \[a_N=\sqrt{|a|^2-a_T^2}\]
                \paragraph{Torsion}
                    \subparagraph{Definition} Let $B=T\times N$. The \textbf{torsion} function of a smooth curve is
                    \[\tau=-\cfrac{dB}{ds}\cdot N\]
                \paragraph{Formulas for Computing Curvature and Torsion}
                    \[\kappa=\cfrac{|v\times a|}{|v|^3}\]
                    \subparagraph{Formula for Torsion}
                        \[\tau=\cfrac{
                            \begin{vmatrix}
                                \dot{x}&\dot{y}&\dot{z}\\
                                \ddot{x}&\ddot{y}&\ddot{z}\\
                                \dddot{x}&\dddot{y}&\dddot{z}\\
                            \end{vmatrix}
                        }{|v\times a|^2}\qquad(\text{if } v\times a\ne 0)\]
                    \subparagraph{Newton's Dot Notation for Derivatives}
                    \[\dot{x}=\cfrac{dx}{dt}\]
                    \[\ddot{x}=\cfrac{d^2x}{dt^2}\]
                    \[\dddot{x}=\cfrac{d^3x}{dt^3}\]
                    and so on.
                \paragraph{Computation Formulas for Curves in Space}
                    \[T=\cfrac{v}{|v|}\]
                    \[N=\cfrac{dT/dt}{|dT/dt|}\]
                    \[B=T\times N\]
                    \[\kappa=\bigg|\cfrac{dT}{ds}\bigg|=\cfrac{|v\times a}{|v|^3}\]
                    \[\tau=-\cfrac{dB}{ds}\cdot N=\cfrac{
                            \begin{vmatrix}
                                \dot{x}&\dot{y}&\dot{z}\\
                                \ddot{x}&\ddot{y}&\ddot{z}\\
                                \dddot{x}&\dddot{y}&\dddot{z}\\
                            \end{vmatrix}
                        }{|v\times a|^2}\]
                    \[a=a_TT+a_NN\]
                    \[a_T=\cfrac{d}{dt}|v|\]
                    \[a_N=\kappa|v|^2=\sqrt{|a|^2-a_T^2}\]
        \subsection{Velocity and Acceleration in Polar Coordinates}
            \paragraph{Motion in Polar and Cylindrical Coordinates}
                \[P(r,\theta)\]
                \[u_r=(\cos\theta)i+(\sin\theta)j\]
                \[u_\theta=-(\sin\theta)i+(\cos\theta)j\]
                \[\cfrac{du_r}{d\theta}=-(\sin\theta)i+(\cos\theta)j=u_\theta\]
                \[\cfrac{du_\theta}{d\theta}=-(\cos\theta)i-(\sin\theta)j=-u_r\]
                \[\dot{u}_r=\cfrac{du_r}{d\theta}\dot{\theta}=\dot{\theta}u_\theta\]
                \[v=\dot{r}=\cfrac{d}{dt}(ru_r)=\dot{r}u_r+r\dot{u}_r=\dot{r}u_r+r\dot{\theta}u_\theta\]
                \[a=\dot{v}=(\ddot{r}u_r+\dot{r}\dot{u}_r)+(\dot{r}\dot{\theta}u_\theta+r\ddot{\theta}u_\theta+r\dot{\theta}\dot{u}_\theta)\]
                \par Extend to the space
                \par \textit{Position:}\quad $\overrightarrow{r}=ru_r+zk$
                \par \textit{Velocity:}\quad $\overrightarrow{v}=\dot{r}u_r+r\dot{\theta}u_\theta+\dot{z}k$
                \par \textit{Acceleration:}\quad $a=(\ddot{r}-r\dot{\theta}^2)u_r+(r\ddot{\theta}+2\dot{r}\dot{\theta})u_\theta+\ddot{z}k$
                The vectors $u_r$, $u_\theta$, $k$ make a right-handed frame in which
                \[u_r\times u_\theta=k\]
                \[u_\theta\times k=u_r\]
                \[k\times u_r=u_\theta\]
            \paragraph{Planets Move in Planes}
                \[F=-\cfrac{GmM}{|r|^2}\cfrac{r}{|r|}\]
                \[\ddot{r}=-\cfrac{GM}{|r|^2}\cfrac{r}{|r|}\]
                \[r\times\ddot{r}=0\]
                \[\cfrac{d}{dt}(r\times\dot{r})=\dot{r}\times\dot{r}+r\times\ddot{r}=r\times\ddot{r}=0\]
                \[r\times\dot{r}=C\]
                for some constant vector $C$.
            \paragraph{Kepler's First Law(Ellipse Law)}
                \[e=\cfrac{r_0v_0^2}{GM}-1\]
                \[r=\cfrac{(1+e)r_0}{1+e\cos\theta}\]
            \paragraph{Kepler's Second Law(Equal Area Law)}
                \[\cfrac{dA}{dt}=\cfrac{1}{2}r^2\dot{\theta}=\cfrac{1}{2}r_0v_0\]
            \paragraph{Kepler's Third Law(Time-Distance Law)}
                \[\cfrac{T^2}{a^3}=\cfrac{4\pi^2}{GM}\]
                \begin{equation}
                    \begin{aligned}
                        \text{Area}&=\int_0^TdA\\
                        &=\int_0^T\cfrac{1}{2}r_0v_0dt\\
                        &=\cfrac{1}{2}Tr_0v_0\\
                    \end{aligned}
                \end{equation}
                \[T=\cfrac{2\pi ab}{r_0v_0}=\cfrac{2\pi a^2}{r_0v_0}\sqrt{1-e^2}\]
                \[r_{\text{max}}=r_0\cfrac{1+e}{1-e}\]
                \[2a=r_0+r_{\text{max}}=\cfrac{2r_0}{1-e}=\cfrac{2r_0GM}{2GM-r_0v_0^2}\]
    
    \newpage
    \section{Partial Derivatives}
        \subsection{Functions of Several Variables}
            \[w=f(x_1,x_2,\cdots,x_n)\]
            \paragraph{Domains and Ranges}
            \paragraph{Functions of Two Variables}
                \subparagraph{Definition} A region in the plane is \textbf{bounded} if it lies inside a disk of finite radius. A region is \textbf{unbounded} if it is not bounded.
            \paragraph{Graphs, Level Curves, and Contours of Functions of Two Variables}
                \subparagraph{Definitions} The set of points in the plane where a function $f(x,y)$ has a constant value $f(x,y)=c$ is called a \textbf{level curve} of $f$. The set of all points $(x,y,f(x,y))$ in space, for $(x,y)$ in the domain of $f$, is called the \textbf{graph} of $f$. The graph of $f$ is also called the \textbf{surface} $z=f(x,y)$.
            \paragraph{Functions of Three Variables}
                \subparagraph{Definition} The set of points $(x,y,z)$ in space where a function of three independent variables has a constant value $f(x,y,z)=c$ is called a \textbf{level surface} of $f$.
                \subparagraph{Definition} A point $(x_0,y_0,z_0)$ in a region $R$ in space is an \textbf{interior point} of $R$ if it is the center of a solid ball that lies entirely in $R$. A point $(x_0,y_0,z_0)$ is a \text{boundary point} of $R$ if every solid ball centered at $(x_0,y_0,z_0)$ contains points that lie outside of $R$ as well as points that lie inside $R$. The \textbf{interior} of $R$ is the set of interior points of $R$. The \textbf{boundary} of $R$ is the set of boundary points of $R$.
                \par A region is \textbf{open} if it consists entirely of interior points. A region is closed if it contains its entire boundary.
            \paragraph{Computer Graphing}
        \subsection{Limits and Continuity in Higher Dimensions}
            \paragraph{Limits for Functions of Two Variables}
                \[\lim\limits_{(x,y)\to(x_0,y_0)}f(x,y)=L\]
                \subparagraph{Properties of Limits of Funcitons of Two Variables}
                    \[\lim\limits_{(x,y)\to(x_0,y_0)}f(x,y)=L\]
                    \[\lim\limits_{(x,y)\to(x_0,y_0)}g(x,y)=M\]
                    \[\lim\limits_{(x,y)\to(x_0,y_0)}(f(x,y)+g(x,y))=L+M\]
                    \[\lim\limits_{(x,y)\to(x_0,y_0)}(f(x,y)-g(x,y))=L-M\]
                    \[\lim\limits_{(x,y)\to(x_0,y_0)}kf(x,y)=kL\]
                    \[\lim\limits_{(x,y)\to(x_0,y_0)}(f(x,y)\cdot g(x,y))=L\cdot M\]
                    \[\lim\limits_{(x,y)\to(x_0,y_0)}\cfrac{f(x,y)}{g(x,y)}=\cfrac{L}{M}\]
                    \[\lim\limits_{(x,y)\to(x_0,y_0)}[f(x,y)]^n=L^n\]
                    \[\lim\limits_{(x,y)\to(x_0,y_0)}\sqrt[n]{f(x,y)}=\sqrt[n]{L}=L^{1/n}\]
            \paragraph{Continuity}
                \subparagraph{Definition} A function $f(x,y)$ is \textbf{continuous at the point} $(x_0,y_0)$ if 
                \par 1. $f$ is defined at $(x_0,y_0)$,
                \par 2. $\lim\limits_{(x,y)\to(x_0,y_0)}f(x,y)$ exists,
                \par 3. $\lim\limits_{(x,y)\to(x_0,y_0)}f(x,y)=f(x_0,y_0)$.
                \par A function is continuous if it is continuous at every point of its domain.
                \subparagraph{Two-Path Test for Nonexistence of a Limit}
                If a function $f(x,y)$ has different limits along two different paths in the domain of $f$ as $(x,y)$ approaches $(x_0,y_0)$, then $\lim\limits_{(x,y)\to(x_0,y_0)}f(x,y)$ does not exists.
                \subparagraph{Continuity of Composites}
                    If $f$ is continuous at $(x_0,y_0)$ and $g$ is a single-variable function continuous at $f(x_0,y_0)$, then the composite function $h=g\circ f$ defined by $h(x,y)=g(f(x,y))$ is continuous at $(x_0,y_0)$.
            \paragraph{Functions of More Than Two Variables}
            \paragraph{Extreme Values of Continuous Functions on Closed, Bounded Sets}
        \subsection{Partial Derivatives}
            \paragraph{Partial Derivatives of a Function of Two Variables}
                \subparagraph{Definition} The Partial derivative of $f(x,y)$ with respect to $x$ at the point of $(x_0,y_0)$ is
                \[\cfrac{\partial f}{\partial x}\bigg|_{(x_0,y_0)}=\lim\limits_{h\to 0}\cfrac{f(x_0+h,y_0)-f(x_0,y_0)}{h}\]
                provided the limit exists.
                \subparagraph{Definition} The Partial derivative of $f(x,y)$ with respect to $y$ at the point of $(x_0,y_0)$ is
                \[\cfrac{\partial f}{\partial y}\bigg|_{(x_0,y_0)}=\cfrac{d}{dy}f(x_0,y)\bigg|_{y=y_0}=\lim\limits_{h\to 0}\cfrac{f(x_0,y_0+h)-f(x_0,y_0)}{h}\]
                provided the limit exists.
            \paragraph{Calculations}
            \paragraph{Functions of More than Two Variables}
            \paragraph{Partial Derivatives and Continuity}
            \paragraph{Second-Orther Partial Derivatives}
                \[\cfrac{\partial^2f}{\partial x^2}=f_{xx}\]
                \[\cfrac{\partial^2f}{\partial y^2}=f_{yy}\]
                \[\cfrac{\partial^2f}{\partial x\partial y}=f_{yx}\]
                \[\cfrac{\partial^2f}{\partial y\partial x}=f_{xy}\]
            \paragraph{The Mixed Derivative Theorem}
                \subparagraph{The Mixed Derivative Theorem}
                \[f_{xy}(a,b)=f_{yx}(a,b)\]
            \paragraph{Partial Derivatives of Still Higher Order}
                \[\cfrac{\partial^3f}{\partial x\partial y^2}=f_{yyx}\]
                \[\cfrac{\partial^4f}{\partial x^2\partial y^2}=f_{yyxx}\]
            \paragraph{Differentiablility}
                \subparagraph{The Increment Theorem for Functions of Two Variables} Suppose that the first partial derivatives of $f(x,y)$ are defined throughout an open region $R$ containing the point $(x_0,y_0)$ and  that $f_x$ and $f_y$ are continuous at $(x_0,y_0)$. Then the change
                \[\Delta z=f(x_0+\Delta x,y_0+\Delta y)-f(x_0,y_0)\]
                in the value of $f$ that results from moving from $(x_0,y_0)$ to another point $(x_0+\Delta x,y_0+\Delta y)$ in $R$ satisfies an equation of the form
                \[\Delta z=f_x(x_0,y_0)\Delta x+f_y(x_0,y_0)\Delta y+\epsilon_1\Delta y+\epsilon_2 \Delta y\]
                in which each of $\epsilon_1$, $\epsilon_2\to 0$ as both $\Delta x,\Delta y\to 0$.
                \subparagraph{Corollary of Theorem} If the partial derivatives $f_x$ and $f_y$ of a function $f(x,y)$ are continuous throughout an open region $R$, then $f$ is differentiable at every point of $R$.
                \subparagraph{Differentiability Implies Continuity} If a funciton $f(x,y)$ is differentiable at $(x_0,y_0)$, then $f$ is continuous at $(x_0,y_0)$.
        \subsection{The Chain Rule}
            \paragraph{Functions of Two Variables}
                \subparagraph{Chain Rule For Functions of One Independent Variable and Two Intermediate Variables} If $w=f(x,y)$ is differentiable and if $x=x(t)$, $y=y(t)$ are differentiable functions of $t$, then the composite $w=f(x(t),y(t))$ is a differentiable function of $t$ and
                \[\cfrac{dw}{dt}=f_x(x(t),y(t))\cdot x'(t)+f_y(x(t,y(t))\cdot y'(t))\]
                or
                \[\cfrac{dw}{dt}=\cfrac{\partial w}{\partial x}\cfrac{dx}{dt}+\cfrac{\partial f}{\partial y}\cfrac{dy}{dt}\]
                \newline
            \[\cfrac{dw}{dt}=\cfrac{\partial w}{\partial x}\cfrac{dx}{dt}+\cfrac{\partial w}{\partial y}\cfrac{dy}{dt}\]
            \paragraph{Function of Three Variables}
                \[\cfrac{dw}{dt}=\cfrac{\partial w}{\partial x}\cfrac{dx}{dt}+\cfrac{\partial w}{\partial y}\cfrac{dy}{dt}+\cfrac{\partial w}{\partial z}\cfrac{dz}{dt}\]
            \paragraph{Functions Defined on Surfaces}
                \subparagraph{Chain Rule for Two Independent Variables and Three Intermediate Variables} Suppose that $w=f(x,y,z),\ x=g(r,s),\ y=h(r,s), z=k(r,s)$. If all for functions are differentiable, then $w$ has partial derivatives with respect to $r$ and $s$, given by the formulas
                    \[\cfrac{\partial w}{\partial r}=\cfrac{\partial w}{\partial x}\cfrac{\partial x}{\partial r}+\cfrac{\partial w}{\partial y}\cfrac{\partial y}{\partial r}+\cfrac{\partial w}{\partial z}\cfrac{\partial z}{\partial r}\]
                \par If $w=f(x,y),\ x=g(r,s),$ and $y=h(r,s)$, then
                \[\cfrac{\partial w}{\partial r}=\cfrac{\partial w}{\partial x}\cfrac{\partial x}{\partial r}+\cfrac{\partial w}{\partial y}\cfrac{\partial y}{\partial r}\]
                \[\cfrac{\partial w}{\partial s}=\cfrac{\partial w}{\partial x}\cfrac{\partial x}{\partial s}+\cfrac{\partial w}{\partial y}\cfrac{\partial y}{\partial s}\]
                \par if $w=f(x)$ and $x=g(r,s)$, then
                \[\cfrac{\partial w}{\partial r}=\cfrac{dw}{dx}\cfrac{\partial x}{\partial r}\]
                \[\cfrac{\partial w}{\partial s}=\cfrac{dw}{dx}\cfrac{\partial x}{\partial s}\]
            \paragraph{Implicit Differentiation Revisited}
                \subparagraph{A Formula for Implicit Differentiation} Suppose that $F(x,y)$ is differentiable and that the equation $F(x,y)=0$ defines $y$ as a differentiable function of $x$. Then at any point where $F_y\ne 0$,
                \[\cfrac{dy}{dx}=-\cfrac{F_x}{F_y}\]
            \paragraph{Functions of Many Variables}
                \[\cfrac{\partial w}{\partial p}=\cfrac{f\partial w}{\partial x}\cfrac{\partial x}{\partial p}+\cfrac{\partial w}{\partial y}\cfrac{\partial y}{\partial p}+\cdots+\cfrac{\partial w}{\partial v}\cfrac{\partial v}{\partial p}\]
        \subsection{Directional Derivatives and Gradient Vectors}
            \paragraph{Directional Derivatives in the Plane}
                \subparagraph{Definition} The derivative of $f$ at $P_0(x_0,y_0)$ in the direction of the unit vector $u=u_1i+u_2j$ is the number
                \[\bigg(\cfrac{df}{ds}\bigg)_{u,P_0}=\lim\limits_{s\to 0}\cfrac{f(x_0+su_1,y_0+su_2)-f(x_0,y_0)}{s}\]
                provided the limit exists.
                \par The \textbf{directional derivative} defined by Equation is also denoted by
                \[(D_uf)_{p_0}\]
            \paragraph{Interpretation of the Directional Derivative}
            \paragraph{Calculation and Gradients}
                \begin{equation}
                    \begin{aligned}
                        (\cfrac{df}{ds})_{u,P_0}&=(\cfrac{\partial f}{\partial x})_{P_0}\cfrac{dx}{ds}+(\cfrac{\partial f}{\partial y})_{P_0}\cfrac{dy}{ds}\\
                        &=(\cfrac{\partial f}{\partial x})_{P_0}u_1+(\cfrac{\partial f}{\partial y})_{P_0}u_2\\
                        &=[(\cfrac{\partial f}{\partial x})_{P_0}i+(\cfrac{\partial f}{\partial y})_{P_0}j]\cdot[u_1i+u_2j]\\
                    \end{aligned}
                \end{equation}
                \subparagraph{Definition} The \textbf{gradient vector (gradient)} of $f(x,y)$ at a point $P_0(x_0,y_0)$ is the vector
                \[\nabla f=\cfrac{\partial f}{\partial x}i+\cfrac{\partial f}{\partial y}j\]
                obtained by evaluating the partial derivatives of $f$ at $P_0$.
                \subparagraph{The Directional Derivative is a Dot Product} If $f(x,y)$ is differentiable in an open region containing $P_0(x_0,y_0)$, then 
                \[\bigg(\cfrac{df}{ds}\bigg)_{u,P_0}=(\nabla f)_{P_0}\cdot u\]
                the dot product of the gradient $\nabla f$ at $P_0$ and $u$. In brief, $D_uf=\nabla f\cdot u$
            \paragraph{Gradients and Tangents to Level Curves}
                \[\nabla f=\cfrac{\partial f}{\partial x}i+\cfrac{\partial f}{\partial y}j\]
                \[\cfrac{dr}{dt}=\cfrac{dg}{dt}i+\cfrac{dh}{dt}j\]
                \[\nabla f\cdot \cfrac{dr}{dt}=0\]
                \subparagraph{Tangent Line to a Level Curve}
                    \[f_x(x_0,y_0)(x-x_0)+f_y(x_0,y_0)(y-y_0)=0\]
            \paragraph{Algebra Rules for Gradients}
                \[\nabla(f+g)=\nabla f+\nabla g\]
                \[\nabla(f-g)=\nabla f-\nabla g\]
                \[\nabla(kf)=k\nabla f\]
                \[\nabla(fg)=f\nabla g+g\nabla f\]
                \[\nabla(\cfrac{f}{g})=\cfrac{g\nabla f-f\nabla g}{g^2}\]
            \paragraph{Functions of Three Variables}
                \[\nabla f=\cfrac{\partial f}{\partial x}i+\cfrac{\partial f}{\partial y}j+\cfrac{\partial f}{\partial z}k\]
                \[D_uf=\nabla f\cdot u=\cfrac{\partial f}{\partial x}u_1+\cfrac{\partial f}{\partial y}u_2+\cfrac{\partial f}{\partial z}u_3\]
                \[D_uf=|\nabla f||u|\cos\theta=|\nabla f|\cos\theta\]
            \paragraph{The Chain Rule for Paths}
                If $r(t)=x(t)i+y(t)j+z(t)k$ is a smooth path $C$, and $w=f(r(t))$ is a scalar function evaluated along $C$.
                \[\cfrac{dw}{dt}=\cfrac{\partial w}{\partial x}\cfrac{dx}{dt}+\cfrac{\partial w}{\partial y}\cfrac{dy}{dt}+\cfrac{\partial w}{\partial z}\cfrac{dz}{dt}\]
                \subparagraph{The Derivative Along a Path}
                    \[\cfrac{d}{dt}f(r(t))=\nabla f(r(t))\cdot r'(t)\]
        \subsection{Tangent Planes and Differentials}
            \paragraph{Tangent Planes and Normal Lines}
                \subparagraph{Definition} The \textbf{tangent plane} at the point $P_0(x_0,y_0,z_0)$ on the level surface $f(x,y,z)=c$ of a differentiable function $f$ is the plane through $P_0$ normal to $\nabla f|_{P_0}$.
                \par The \textbf{normal line} of the surface at $P_0$ is the line through $P_0$ parallel to $\nabla f|_{P_0}$.
                \subparagraph{Tangent Plane to $f(x,y,z)=c$ at $P_0(x_0,y_0,z_0)$}
                \[f_x(P_0)(x-x_0)+f_y(P_0)(y-y_0)+f_z(P_0)(z-z_0)=0\]
                \subparagraph{Normal Line to $f(x,y,z)=c$ at $P_0(x_0,y_0,z_0)$}
                \[x=x_0+f_x(P_0)t\]
                \[y=y_0+f_y(P_0)t\]
                \[z=z_0+f_z(P_0)t\]
                \subparagraph{Plane Tangent to a Surface $z=f(x,y)$ at $(x_0,y_0,f(x_0,y_0))$}
                \[f_x(x_0,y_0)(x-x_0)+f_y(x_0,y_0)(y-y_0)-(z-z_0)=0\]
            \paragraph{Estimating Change in a Specific Direction}
                \[df=(\nabla f|_{P_0}\cdot u) ds\]
            \paragraph{How to Linearize a Function of Two Variables}
                The \textbf{linearization} of a function $f(x,y)$ at a point $(x_0,y_0)$ where $f$ is differentiable is the function
                \[L(x,y)=f(x_0,y_0)+f_x(x_0,y_0)(x-x_0)+f_y(x_0,y_0)(y-y_0)\]
                \subparagraph{The Error in the Standard Linear Approximation}
                \[|E(x,y)|\le\cfrac{1}{2}M(|x-x_0|+|y-y_0|)^2\]
                \subparagraph{Definition} If we move from $(x_0,y_0)$ to a pint $(x_0+dx,y_0+dy)$ nearby, the resulting change
                \[df=f_x(x_0,y_0)dx+f_y(x_0,y_0)dy\]
                in the linearization of $f$ is called the \textbf{total differential of $\bf{f}$}.
            \paragraph{Differentials}
                \[df=f_x(x_0,y_0)dx+f_y(x_0,y_0)dy\]
            \paragraph{Functions of More Than Two Variables}
                \[L(x,y,z)=f(P_0)+f_x(P_0)(x-x_0)+f_y(P_0)(y-y_0)+f_z(P_0)(z-z_0)\]
        \subsection{Extreme Value and Saddle Points}
            \paragraph{Derivative Tests for Local Extreme Values}
                \subparagraph{Definition} Let $f(x,y)$ be defined on a region $R$ containing the point $(a,b)$. Then
                \par 1. $f(a,b)$ is a \textbf{local maximum} value of $f$ if $f(a,b)\ge f(x,y)$ for all domain points $(x,y)$ in an open disk centered at $(a,b)$.
                \par 2. $f(a,b)$ is a \textbf{local minimum} value of $f$ if $f(a,b)\le f(x,y)$ for all domain points $(x,y)$ in an open disk centered at $(a,b)$.
                \subparagraph{First Derivative Test for Local Extreme Values} If $f(x,y)$ has a local maximum or minimum value at an interior point $(a,b)$ of its domain and if the first partial derivatives exist there, then $f_x(a,b)=0$ and $f_y(a,b)=0$.
                \subparagraph{Interior point} An interior point of the domain of a function $f(x,y)$ where both $f_x$ and $f_y$ are zero or where one or both of $f_x$ and $f_y$ do not exist is a \textbf{critical point of} $f$.
                \subparagraph{Saddle Point} A differentiable function $f(x,y)$ has a saddle point at a critical point (a,b) if in every open disk centered at $(a,b)$ there are domain points $(x,y)$ where $f(x,y)>f(a,b)$ and domain points $(x,y)$ where $f(x,y)<f(a,b)$. The corresponding point $(a,b,f(a,b))$ on the surface $z=f(x,y)$ is called a saddle point of the surface.
                \subparagraph{Second Derivative Test for Local Extreme Values} Suppose that $f(x,y)$ and its first and second partial derivatives are continuous throughout a disk centered at $(a,b)$ and that $f_x(a,b)=f_y(a,b)=0$. Then
                \par 1. $f$ has a \textbf{local maximum} at $(a,b)$ if $f_{xx}<0$ and $f_{xx}f_{yy}-f_{xy}^2>0$ at $(a,b)$.
                \par 2. $f$ has a \textbf{local minimum} at $(a,b)$ if $f_{xx}>0$ and $f_{xx}f_{yy}-f_{xy}^2>0$ at $(a,b)$.
                \par 3. $f$ has a \textbf{local maximum} at $(a,b)$ if $f_{xx}f_{yy}-f_{xy}^2<0$ at $(a,b)$.
                \par 4. \textbf{The test is inconclusive} at $(a,b)$ if $f_{xx}f_{yy}-f_{xy}^2=0$ at $(a,b)$.\\
                \textbf{Hessian} or \textbf{Discriminant} of $f$ is
                \[f_{xx}f_{yy}-f_{xy}^2=
                \begin{vmatrix}
                    f_{xx}&f_{xy}\\
                    f_{xy}&f_{yy}\\
                \end{vmatrix}\]
            \paragraph{Absolute Maxima and Minima on Closed Bounded Regions}
                \text{}\\
                \par 1. \textit{List the interior points of $R$}.
                \par 2. \textit{List the boundary points of $R$}.
                \par 3. \textit{Look through the list} for the maximum and minimum values of $f$.
        \subsection{Lagrange Multiplier}
            \paragraph{Constrained Maxima and Minima}
            \paragraph{The Method of Lagrange Multiplier}
                \[\nabla f=\lambda \nabla g\]
                for some scalar $\lambda$ called a \textbf{Lagrange multiplier}.
                \subparagraph{The Orthogonal Gradient Theorem} Suppose that $f(x,y,z)$ is differentiable in a region whose interior contains a smooth curve
                \[C:\quad r(t)=x(t)i+y(t)j+z(t)k\]
                If $P_0$ is a point on $C$ where $f$ has a local maximum or minimum relative to its value on $C$, then $\nabla f$ is orthogonal to $C$ at $P_0$.
                \subparagraph{Corollary} At the points on a smooth curve $r(t)=x(t)i+y(t)j$ where a differentiable function $f(x,y)$ takes on its local maxima and minima relative to its value on the curve, $\nabla f\cdot r'=0$.
                \subparagraph{The Method of Lagrange Multipliers}
                    Suppose that $f(x,y,z)$ and $g(x,y,z)$ are differentiable and $\nabla g\ne 0$ when $g(x,y,z)=0$. To find the local maximum and minimum values of $f$ subject to thee constraint $g(x,y,z)=0$(if these exist), find the values of $x,y,z$, and $\lambda$ that simultaneously satisfy the equations 
                    \[\nabla f=\lambda\nabla g\qquad\text{and}\qquad g(x,y,z)=0\]
                    For functions of two independent variables, the condition is similar, but without the variable $z$.
            \paragraph{Lagrange Multipliers with Two Constraints}
                \[\nabla f=\lambda\nabla g_1+\mu\nabla g_2,\qquad g_1(x,y,z)=0,\qquad g_2(x,y,z)=0\]
        \subsection{Taylor's Formula for Two Variables}
            \paragraph{Derivation of the Second Derivative Test}
                \[x=a+th,\qquad y=b+tk\qquad 0\le t\le 1\]
                \[F(t)=f(a+th,b+tk)\]
                \[F'(t)=f_x\cfrac{dx}{dt}+f_y\cfrac{dy}{dt}=hf_x+kf_y\]
                \[F''(t)=\cfrac{\partial F'}{\partial x}\cfrac{dx}{dt}+\cfrac{\partial F'}{\partial y}\cfrac{dy}{dt}=\cfrac{\partial}{\partial x}(hf_x+kf_y)\cdot h+\cfrac{\partial}{\partial y}(hf_x+kf_y)\cdot k\]
                \[=h^2f_{xx}+2hkf_{xy}+k^2f_{yy}\]
                \[F(1)=F(0)+F'(0)(1-0)+F''(c)\cfrac{(1-0)^2}{2}\]
                \[=F(0)+F'(0)+\cfrac{1}{2}F''(c)\]
                for some $c$ between 0 and 1.
                \[f(a+h,b+k)=f(a,b)+hf_x(a,b)+kf_y(a,b)+\cfrac{1}{2}(h^2f_{xx}+2hkf_{xy}+k^2f_{yy})\bigg|_{(a+ch,b+ck)}\]
                \[Q(c)=(h^2f_{xx}+2hkf_{xy}+k^2f_{yy})\bigg|_{(a+ch,b+ck)}\]
                \[Q(0)=h^2f_{xx}(a,b)+2hkf_{xy}(a,b)+k^2f_{yy}(a,b)\]
                \[f_{xx}Q(0)=(hf_{xx}+kf_{xy})^2+(f_{xx}f_{yy}-f_{xy}^2)k^2\]
                \par 1. If $f_{xx}<0$ and $f_{xx}f_{yy}-f_{xy}^2>0$ at $(a,b)$, then $Q(0)<0$ for all sufficiently small nonzero values of $h$ and $k$, and $f$ has a \textit{local maximum} value at $(a,b)$.
                \par 2. If $f_{xx}>0$ and $f_{xx}f_{yy}-f_{xy}^2>0$ at $(a,b)$, then $Q(0)>0$ for all sufficiently small nonzero values of $h$ and $k$, and $f$ has a \textit{local minimum} value at $(a,b)$.
                \par 3. If $f_{xx}f_{yy}-f_{xy}^2<0$ at $(a,b)$, there are combinations of arbitrarily small nonzero values of $h$ and $k$ for which $Q(0)>0$, and other values for which $Q(0)<0$. Arbitrarily close to the point $P_0(a,b,f(a,b))$ on the surface $z=f(x,y)$ there are points above $P_0$ and points below $P_0$, so $f$ has a \textit{saddle point} at $(a,b)$.
                \par 4. If $f_{xx}f_{yy}-f_{xy}^2=0$, another test is needed.
            \paragraph{Taylor's Formula for FUnctions of Two Variables}
                \subparagraph{Taylor's Formula for $f(x,y)$ at the Point $(a,b)$}
                \[f(a+h,b+k)=f(a,b)+(hf_x+kf_y)\bigg|_{(a,b)}+\cfrac{1}{2!}(h^2f_{xx}+2hkf_{xy}+k^2f_{yy})\bigg|_{(a,b)}\]
                \[+\cfrac{1}{3!}(h^3f_{xxx}+3h^2kf_{xxy}+3hk^2f_{xyy}+k^3f_{yyy})\bigg|_{(a,b)}+\cdots\]
                \[+\cfrac{1}{n!}(h\cfrac{\partial}{\partial x}+k\cfrac{\partial}{\partial y})^nf\bigg|_{(a,b)}+\cfrac{1}{(n+1)!}(h\cfrac{\partial}{\partial x}+k\cfrac{\partial}{\partial y})^{n+1}f\bigg|_{a+ch,b+ck}\]
                \subparagraph{Taylor's Formula for $f(x,y)$ at the Origin}
                \[f(x,y)=f(0,0)+xf_x+yf_y+\cfrac{1}{2!}(x^2f_{xx}+2xyf_{xy}+y^2f_{yy})\]
                \[+\cfrac{1}{3!}(x^3f_{xxx}+3x^2yf_{xxy}+3xy^2f_{xyy}+y^3f_{yyy})+\cdots\]
                \[+\cfrac{1}{n!}(x^n\cfrac{\partial^nf}{\partial x^n}+nx^{n-1}y\cfrac{\partial^nf}{\partial x^{n-1}\partial y}+\cdots+y^n\cfrac{\partial ^nf}{\partial y^n})\]
                \[+\cfrac{1}{(n+1)!}(x^{n+1}\cfrac{\partial^{n+1}f}{\partial x^{n+1}}+(n+1)x^ny\cfrac{\partial^{n+1}f}{\partial x^n\partial y}+\cdots+y^{n+1}\cfrac{\partial^{n+1}f}{\partial y^{n+1}})\bigg|_{(cx,cy)}\]
        \subsection{Partial Derivatives with Constrained Variables}
            \paragraph{Decide Which Variables Are Dependent and Which are Independent}
            \paragraph{How to Find $\partial w/\partial x$ When the Variables in $w=f(x,y,z)$ Are Constrained by Another Equation}
            \paragraph{Notation}
            \paragraph{Arrow Diagrams}
            \[w=x^2+y-z+\sin t\quad\text{and}\quad x+y=t\]
            \[\begin{pmatrix}
                x\\
                y\\
                z\\
            \end{pmatrix}\quad\to\quad
            \begin{pmatrix}
                x\\
                y\\
                z\\
                t\\
            \end{pmatrix}\quad\to\quad w\]
    
    \newpage
    \section{Multiple Integrals}
        \subsection{Double and Iterated Integrals over Rectangles}
\end{document}